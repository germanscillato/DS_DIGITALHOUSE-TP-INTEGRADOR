{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba54bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install webdriver-manager\n",
    "# !pip install bs4\n",
    "# !pip install xgboost\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import requests\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760ce7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_true_location_eliel = '~/Downloads/DHDS/TP Final/git_fake_news/data/True.csv'\n",
    "# data_fake_location_eliel = '~/Downloads/DHDS/TP Final/git_fake_news/data/Fake.csv'\n",
    "# # data_true_location_lucas = 'C:\\\\Users\\\\Lucas Choconi\\\\Documents\\\\DH\\\\Datasets\\\\fakenews\\True.csv'\n",
    "# # data_fake_location_lucas = 'C:\\\\Users\\\\Lucas Choconi\\\\Documents\\\\DH\\\\Datasets\\\\fakenews\\Fake.csv'\n",
    "# # data_true_location_ger = ''\n",
    "# # data_fake_location_ger = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d24083b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "\n",
       "                date  real  \n",
       "0  December 31, 2017  Fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset from https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\n",
    "file_names_true = pd.read_csv('./data/True.csv')\n",
    "file_names_true['real'] = 'True'\n",
    "file_names_fake = pd.read_csv('./data/Fake.csv')\n",
    "file_names_fake['real'] = 'Fake'\n",
    "data = file_names_fake.append(file_names_true, ignore_index=True)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e7d440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing</td>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing --- Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                text  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing --- Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.   \n",
       "\n",
       "  subject               date  real  \n",
       "0    News  December 31, 2017  Fake  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unimos el título y el contenido del artículo en una sola columna\n",
    "data['text'] = data['title'] + ' --- ' + data['text'] \n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  display(data.head(1))\n",
    "data = data.drop(columns = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90d1791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9358</th>\n",
       "      <td>https://100percentfedup.com/served-roy-moore-v...</td>\n",
       "      <td>politics</td>\n",
       "      <td>https://100percentfedup.com/served-roy-moore-v...</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   subject  \\\n",
       "9358  https://100percentfedup.com/served-roy-moore-v...  politics   \n",
       "\n",
       "                                                   date  real  \n",
       "9358  https://100percentfedup.com/served-roy-moore-v...  Fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Algunas filas contienen datos erróneos que impiden transformar la columna 'date' a formato fecha\n",
    "wrong_row = data['date'] == 'https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/'\n",
    "data[wrong_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0744bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las filas que no pueden ser correctamente transformadas a datetime serán utilizadas en una máscara booleana\n",
    "# para poder eliminarlas\n",
    "data['date'] = data['date'].apply(lambda x: pd.to_datetime(x, infer_datetime_format = True)\\\n",
    "                                                           if re.search('[a-z-A-Z]+ [0-9]+, [0-9]{4}', x) else\n",
    "                                                            'delete')\n",
    "delete_mask = data['date'] != 'delete'\n",
    "data = data[delete_mask]\n",
    "data = data.reset_index()\n",
    "data['date'] = data['date'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6013763",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3902783948.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_122227/3902783948.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    data_fake = data[data['real'] == 'Fake']\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Separamos los datos en las filas que contienen noticias falsas de las que son reales\n",
    "data_real = data[data['real'] == 'True'\n",
    "\n",
    "data_fake = data[data['real'] == 'Fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d437ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Averiguamos cuál es el largo promedio de las noticias falsas y de las reales y hacemos un gráfico comparativo\n",
    "#data['text_len'] = data['text'].apply(lambda x: len(word_tokenize(x)))\n",
    "#real_len_avg = data_real['text_len'].mean()\n",
    "#fake_len_avg = data_fake['text_len'].mean()\n",
    "#len_avgs = [real_len_avg, fake_len_avg]\n",
    "#plt.bar(['Real', 'Fake'], len_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Lista con todas las palabras en inglés https://github.com/dwyl/english-words/blob/master/words.txt\n",
    "all_eng_words = pd.read_csv('./data/all_english_words.csv')\n",
    "all_eng_words = pd.Series(all_eng_words.iloc[:,0])\n",
    "all_eng_words = all_eng_words.apply(lambda x: x.lower() if type(x) == str else x)\n",
    "\n",
    "def words_percentage(text):\n",
    "    # Devuelve el porcentaje de palabras que posee un texto que están dentro de un diccionario\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words] \n",
    "    words_in_dict = [word for word in words if word in all_eng_words.values]\n",
    "    words_in_dict_percentage = len(words_in_dict) * 100 / len(words)\n",
    "    return words_in_dict_percentage\n",
    "\n",
    "tqdm.pandas()\n",
    "data['words_in_dict_pct'] = data['text'].progress_apply(words_percentage)\n",
    "'''\n",
    "'''Esta celda demora 35hs en ejecutarse.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(12, 4)})\n",
    "sns.histplot(data = data, x = 'subject', hue = 'real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como hay valores diferentes en la columna \"subject\" para las noticias reales y las falsas borramos esa columna\n",
    "data = data.drop(columns = 'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c55e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hacemos un gráfico mostrando el conteo de cuántas noticias falsas y cuántas verdaderas se publicaron \n",
    "# por mes en el dataset \n",
    "ax = data_real['date'].groupby([data_real[\"date\"].dt.year, data_real[\"date\"].dt.month]).count().plot.line(label = 'Real news')\n",
    "ax = data_fake['date'].groupby([data_fake[\"date\"].dt.year, data_fake[\"date\"].dt.month]).count().plot.line(label = 'Fake news')\n",
    "ax.set(xlabel=None)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a990792",
   "metadata": {},
   "source": [
    "El pico de noticias verdaderas publicadas en Noviembre del 2016 coincide con las elecciones presidenciales de Estados Unidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El dataset no valores nulos así que damos for finalizada la limpieza\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.2)\n",
    "testing_sample_size = 100 # Algunas operaciones se demoran mucho tiempo, este tamaño pequeño es para agilizarlas\n",
    "sample_size = 5000\n",
    "data_train = data_train.sample(testing_sample_size)\n",
    "data_test = data_test.sample(testing_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "stopwords_en_porter = [porter.stem(x) for x in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_tokenizer(text):\n",
    "    ''' Esta función se utiliza cuando se sobre escribe el proceso de tokenización en la clase CountVectorizer\n",
    "    para que se pueda obtener también la raíz de la palabra.'''\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words]  # remover los caracteres especiales\n",
    "    words = [word.replace(' ', '') for word in words] #remover los espacios en blanco\n",
    "    words = [word for word in words if word != '']\n",
    "    words = [PorterStemmer().stem(word) for word in words] # obtener la raiz de las palabras\n",
    "    words = [word for word in words if word not in stopwords_en_porter] # remover las stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d056e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Cuando se pasa un callable como argumento para modificar el comportamiento predeterminado de alguna etapa\n",
    "# del preprocesamiento del texto del CountVectorizer se emite un warning. Por lo que investigamos a las\n",
    "# stopwords que se pasen como argumento se les va a aplicar la misma función que se pasa como callable para\n",
    "# pre procesarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= CountVectorizer(stop_words = stopwords_en, lowercase = True, strip_accents='unicode', \\\n",
    "                            min_df = 3, ngram_range=(1, 3));\n",
    "vectorizer.fit(data_train['text'])\n",
    "vectorizer_matrix = vectorizer.transform(data_train['text'])\n",
    "vectorizer_df = pd.DataFrame(vectorizer_matrix.todense(), columns=vectorizer.get_feature_names())\n",
    "vectorizer_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa3faf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Esta celda se demora un poco en ejecutarse\n",
    "# La ventaja de este CountVectorizer customizado es que permite obtener la raíz de la palabra\n",
    "customized_vectorizer= CountVectorizer(lowercase = True, strip_accents='unicode', \\\n",
    "                           tokenizer = porter_tokenizer, ngram_range = (1, 3), min_df = 3);\n",
    "customized_vectorizer.fit(data_train['text'])\n",
    "customized_vectorizer_matrix = customized_vectorizer.transform(data_train['text'])\n",
    "customized_vectorizer_df = pd.DataFrame(customized_vectorizer_matrix.todense(), columns=customized_vectorizer.get_feature_names())\n",
    "customized_vectorizer_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f863532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = TfidfTransformer().fit_transform(vectorizer_matrix)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns = vectorizer.get_feature_names())\n",
    "tfidf_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    ''' Esta función se utiliza para hacer un cambio de base y lograr explicar el porcentaje de los\n",
    "    datos que se pasa en el argumento goal_var.\n",
    "    Fuente https://chrisalbon.com/code/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    "    Fue modificada para que hiciera un gráfico de la varianza explicativa del modelo de acuerdo al número\n",
    "    de componentes'''\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    # Listas del nuḿero de componentes y de la explicación de varianza alcanzada para poder graficarlas\n",
    "    num_components = list(range(1, len(var_ratio) + 1))\n",
    "    variances = []\n",
    "    # For the explained variance of each feature:\n",
    "    found_n_components = False\n",
    "    for explained_variance in var_ratio:\n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        # Agregar la varianza explicada al listado \n",
    "        variances.append(total_variance)\n",
    "        # If we reach our goal level of explained variance and we haven't reached our desired variance\n",
    "        if total_variance >= goal_var and found_n_components == False:\n",
    "            desired_n_components = n_components\n",
    "            found_n_components = True\n",
    "            desired_variance = total_variance\n",
    "    plt.plot(num_components, variances)\n",
    "    plt.scatter(desired_n_components, desired_variance, c = 'r')\n",
    "    plt.xlabel('Número de componentes')\n",
    "    plt.ylabel('Razón de la explicación de la varianza')\n",
    "    plt.hlines(desired_variance, 0, desired_n_components, 'r', 'dashed')\n",
    "    plt.vlines(desired_n_components, 0, desired_variance, 'r', 'dashed')\n",
    "        \n",
    "    # Return the desired number of components\n",
    "    return desired_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda demora en ejecutarse\n",
    "# Tener en cuenta que si el número de filas de la matrix esparsa es menor al número de componentes elegido\n",
    "# como hiperparámetro, una vez que se haya realizado la transformación SVD el número de componentes \n",
    "# obtenido será igual al número de filas de la matriz esparsa, y no será el número de componentes\n",
    "# elegido como hiperpárametro\n",
    "test_svd = TruncatedSVD(n_components = tfidf_matrix.shape[1] - 1);\n",
    "test_svd.fit(tfidf_matrix)\n",
    "tsvd_var_ratios = test_svd.explained_variance_ratio_\n",
    "desired_threshold = 0.95\n",
    "n_components = select_n_components(tsvd_var_ratios, desired_threshold)\n",
    "n_components_dif = tfidf_df.shape[1] - n_components\n",
    "print(f'Número de componentes antes del cambio de base: {tfidf_df.shape[1]}')\n",
    "print(f'Número de componentes después del cambio de base: {n_components}')\n",
    "print(f'Hay {n_components_dif} componentes menos gracias al cambio de base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e73f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda demora en ejecutarse\n",
    "svd = TruncatedSVD(n_components = n_components)\n",
    "svd_matrix = svd.fit_transform(tfidf_matrix)\n",
    "svd_df = pd.DataFrame(svd_matrix)\n",
    "svd_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63555b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text'].sample(1000)\n",
    "y = data['real'].sample(1000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96366c48",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasos = [('vectorizador', CountVectorizer(lowercase = True, strip_accents='unicode',\\\n",
    "                                          tokenizer = porter_tokenizer)), \n",
    "         ('tfidf', TfidfTransformer()), \n",
    "         ('cambio_base', TruncatedSVD(n_components)),\n",
    "         ('modelo', MultinomialNB())]\n",
    "pipeline = Pipeline(pasos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'vectorizador__ngram_range': [(1, 3), (2, 3), (3, 3), (1, 4), (2, 4), (3, 4), (4, 4)],\n",
    "               'vectorizador__min_df': range(1, 4),\n",
    "               'tfidf__norm': ['l1', 'l2'],\n",
    "               'cambio_base': [TruncatedSVD(), None],\n",
    "               #'cambio_base__n_componentes': [n_components, X_train.shape[1] - 1],\n",
    "               'modelo': [MultinomialNB(), LogisticRegression(), DecisionTreeClassifier(), ExtraTreesClassifier()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Este toma menos tiempo\n",
    "param_grid2 = [{#'vectorizador__ngram_range': [(1, 3), (2, 3), (3, 3), (1, 4), (2, 4), (3, 4), (4, 4)],\n",
    "               #'vectorizador__min_df': range(1, 4),\n",
    "               #'tfidf__norm': ['l1', 'l2'],\n",
    "               'cambio_base': [TruncatedSVD(), None],\n",
    "               #'cambio_base__n_componentes': [n_components, X_train.shape[1] - 1],\n",
    "               'modelo': [MultinomialNB(), LogisticRegression()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apróximadamente toma 10 min la ejecución con un sample de 1000 records y param_grid2\n",
    "folds=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "grid = GridSearchCV(pipeline, param_grid2, cv = folds)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa69ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed0c7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab31fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd805c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask_pos = np.array(Image.open(path.join(d, \"Thumbs.png\")))\n",
    "mask_neg = np.array(Image.open(path.join(d, \"Thumbsdw.png\")))\n",
    "\n",
    "text_fake = \" \".join(text for text in file_names[file_names['real']=='Fake'].text)\n",
    "text_true = \" \".join(text for text in file_names[file_names['real']=='True'].text)\n",
    "\n",
    "wordcloud_fake = WordCloud(stopwords=stopwords_en,  mask=mask_neg, max_words=500, background_color=\"white\").generate(text_fake)\n",
    "wordcloud_true = WordCloud(stopwords=stopwords_en, mask=mask_pos, max_words=500, background_color=\"white\").generate(text_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    blues = \"hsl(215,100%%, %d%%)\" % np.random.choice(np.arange(25, 65, 5, dtype=int))\n",
    "    reds = \"hsl(0,100%%, %d%%)\" % np.random.choice(np.arange(40, 80, 5, dtype=int))\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 0:\n",
    "        return blues\n",
    "    else:\n",
    "        return reds\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20,10))  \n",
    "\n",
    "axs[1].imshow(wordcloud_fake.recolor(color_func=blue_color_func),interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].title.set_text('FAKE')\n",
    "axs[0].imshow(wordcloud_true.recolor(color_func=blue_color_func),interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].title.set_text('TRUE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38416dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review_text, tokenizer, stemmer, stopwords):    \n",
    "    \n",
    "    #tokens (eliminamos todos los signos de puntuación)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    \n",
    "    # stemming: raiz y minúsculas:\n",
    "    stem_words = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "    # eliminamos stopwords (ya pasaron por stem)\n",
    "    clean_words = [x for x in stem_words if x not in stopwords]\n",
    "    \n",
    "    result = \" \".join(clean_words)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "englishStemmer = SnowballStemmer(\"english\")\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en.append('reuter')\n",
    "stopwords_en.append('said')\n",
    "stopwords_en.append('Reuters')\n",
    "stopwords_en.append('via')\n",
    "stopwords_en.append('imag')\n",
    "stopwords_en.append('https')\n",
    "stopwords_en.append('com')\n",
    "stopwords_en.append('one')\n",
    "stopwords_en.append('u')\n",
    "stopwords_en.append('also')\n",
    "stopwords_en.append('would')\n",
    "stopwords_en.append('featur')\n",
    "stopwords_en.append('pic')\n",
    "stopwords_en.append('us')\n",
    "stopwords_en.append('wednesday')\n",
    "stopwords_en.append('friday')\n",
    "stopwords_en.append('monday')\n",
    "stopwords_en.append('tuesday')\n",
    "stopwords_en.append('saturday')\n",
    "stopwords_en.append('sunday')\n",
    "stopwords_en.append('thursday')\n",
    "stopwords_en.append('getti')\n",
    "stopwords_en.append('read')\n",
    "stopwords_en.append('gop')\n",
    "stopwords_en.append('watch')\n",
    "stopwords_en.append('donald')\n",
    "stopwords_en.append('trump')\n",
    "stopwords_en.append('hillari')\n",
    "stopwords_en.append('mr')\n",
    "stopwords_en.append('accord')\n",
    "stopwords_en.append('america')\n",
    "stopwords_en.append('seem')\n",
    "stopwords_en.append('youtub')\n",
    "stopwords_en.append('21st')\n",
    "stopwords_en_stem = [englishStemmer.stem(x) for x in stopwords_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "clean_train = data_train.text.progress_apply(lambda x: clean_review(x, tokenizer, englishStemmer, stopwords_en_stem))\n",
    "clean_test = data_test.text.progress_apply(lambda x: clean_review(x, tokenizer, englishStemmer, stopwords_en_stem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(clean_train)\n",
    "X_train_sparse = count_vectorizer.transform(clean_train)\n",
    "X_test_sparse = count_vectorizer.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a04d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_train = data_train.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80076db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test_sparse.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "y_test = data_test.real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba22f7",
   "metadata": {},
   "source": [
    "#### Modelo de Reg. Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f66c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(C = 0.05, solver=\"newton-cg\", penalty=\"l2\")\n",
    "reg_model.fit(X_train, y_train)\n",
    "reg_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myColors = ((0.90, 0.96, 1, 1), (0.70, 0.87, 1, 1), (0, 0.40, 0.75, 0.88))\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', myColors,10)\n",
    "cm = confusion_matrix(y_test, reg_model.predict(X_test))\n",
    "axis_labels = np.sort(y_test.unique())\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1134e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coef_reg_model = pd.DataFrame(np.transpose(reg_model.coef_), index=X_train.columns, columns=['Coef'])\n",
    "Coef_reg_model['coef_abs'] = abs(Coef_reg_model['Coef'])\n",
    "Coef_reg_model = Coef_reg_model.reset_index(-1)\n",
    "Coef_reg_model['words'] = Coef_reg_model['index'] \n",
    "Coef_reg_model['freq'] = (round(Coef_reg_model['coef_abs'] / Coef_reg_model.coef_abs.sum() * Coef_reg_model.shape[0] * 1000 , 0)).apply(lambda x: int(x))\n",
    "Coef_reg_model['odds_direction'] = Coef_reg_model['Coef'].apply(lambda x: 'Negative' if x<0 else 'Positive')\n",
    "Coef_reg_model_positive = Coef_reg_model[Coef_reg_model['odds_direction']=='Positive']\n",
    "Coef_reg_model_positive = Coef_reg_model_positive.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)\n",
    "Coef_reg_model_negative = Coef_reg_model[Coef_reg_model['odds_direction']=='Negative']\n",
    "Coef_reg_model_negative = Coef_reg_model_negative.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d48e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_pos = Coef_reg_model_positive.set_index('words').T.to_dict('index')\n",
    "tuples_neg = Coef_reg_model_negative.set_index('words').T.to_dict('index')\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask_pos = np.array(Image.open(path.join(d, \"Thumbs.png\")))\n",
    "mask_neg = np.array(Image.open(path.join(d, \"Thumbsdw.png\")))\n",
    "\n",
    "wordcloud_positive = WordCloud(max_words=500, mask=mask_pos, background_color=\"white\").generate_from_frequencies(dict(tuples_pos['freq']))\n",
    "wordcloud_negative = WordCloud(max_words=500, mask=mask_neg,background_color=\"white\").generate_from_frequencies(dict(tuples_neg['freq']))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20,10))\n",
    "# Display the generated image:\n",
    "axs[0].imshow(wordcloud_positive.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].title.set_text('Positive')\n",
    "axs[1].imshow(wordcloud_negative.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].title.set_text('Negative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57141e8a",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a76c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "NB_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ced1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, NB_model.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749de777",
   "metadata": {},
   "source": [
    "## Modelo de Reg. Logistica Truncando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c60537",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 200);\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00a74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(svd.explained_variance_ratio_.sum())\n",
    "svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_svd = LogisticRegression(C = 0.05, solver=\"newton-cg\", penalty=\"l2\")\n",
    "reg_model_svd.fit(X_train_svd, y_train)\n",
    "reg_model_svd.score(X_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e689984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, reg_model_svd.predict(X_test_svd))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3531080",
   "metadata": {},
   "source": [
    "## Modelo de Reg. Logistica con conjunto de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba67db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer_bigram = CountVectorizer(ngram_range = (1, 2))\n",
    "count_vectorizer_bigram.fit(clean_train)\n",
    "X_train_bigram_sparse = count_vectorizer_bigram.transform(clean_train)\n",
    "X_test_bigram_sparse = count_vectorizer_bigram.transform(clean_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e28944",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_bigram = LogisticRegression(C = 0.05, solver=\"newton-cg\", penalty=\"l2\")\n",
    "reg_model_bigram.fit(X_train_bigram_sparse, y_train)\n",
    "reg_model_bigram.score(X_test_bigram_sparse, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03175d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, reg_model_bigram.predict(X_test_bigram_sparse))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0010f7fa",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ebede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=4)\n",
    "my_tree.fit(X_train, y_train)\n",
    "my_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db62cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, my_tree.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1e503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "tree.plot_tree(my_tree,feature_names = X_train.columns,filled=True,rounded=True, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a836b",
   "metadata": {},
   "source": [
    "## Random Decision Tree Classifier and Extra random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''random_forest = RandomForestClassifier(n_estimators=100, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n",
    "random_forest.fit(X_train, y_train)\n",
    "random_forest.score(X_test, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a530ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cm = confusion_matrix(y_test, random_forest.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab396e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''extra_random_forest = ExtraTreesClassifier(n_estimators=100, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n",
    "extra_random_forest.fit(X_train, y_train)\n",
    "extra_random_forest.score(X_test, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83de6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cm = confusion_matrix(y_test, extra_random_forest.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1cbc2b",
   "metadata": {},
   "source": [
    "## Boosting de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34901454",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''base_classifier = ExtraTreesClassifier(n_estimators=100, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n",
    "\n",
    "boost_tree = AdaBoostClassifier(base_estimator = base_classifier, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8,                                       \n",
    "                            random_state = 127)\n",
    "boost_tree.fit(X_train, y_train) \n",
    "boost_tree.score(X_test, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9586a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cm = confusion_matrix(y_test, boost_tree.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gb_classifier = GradientBoostingClassifier(loss = 'deviance',\n",
    "                                           learning_rate=0.6,\n",
    "                                           n_estimators = 200,\n",
    "                                           subsample=1,\n",
    "                                           criterion='mse',\n",
    "                                           random_state = 127)\n",
    "\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "gb_classifier.score(X_test, y_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cm = confusion_matrix(y_test, gb_classifier.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f397c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''y_train_xg = y_train.apply(lambda x: 1 if x=='Fake' else 0)\n",
    "y_test_xg = y_test.apply(lambda x: 1 if x=='Fake' else 0)\n",
    "model_xg = XGBClassifier(n_jobs=-1, use_label_encoder=False, n_estimators= 100, max_depth=4, learning_rate= 0.05)\n",
    "model_xg.fit(X_train,y_train_xg)\n",
    "model_xg.score(X_test, y_test_xg)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cm = confusion_matrix(y_test_xg, model_xg.predict(X_test))\n",
    "sns.heatmap(cm, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f1402",
   "metadata": {},
   "source": [
    "## A traves de webscraping se buscan noticias para obtener más datos de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dde34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=chrome_options)\n",
    "\n",
    "def get_text_from_url_bbc(url):\n",
    "    wd.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(wd.page_source)\n",
    "    text = soup.find_all('div', {'data-component': 'text-block'}) #(?P<text>\\>(.*?)\\<)\n",
    "    text = reversed(text) \n",
    "    text2 = \"\"\n",
    "    for i in text:\n",
    "        try:\n",
    "            text2 = i.text+text2\n",
    "        except:\n",
    "            text2 = \"\"+text2\n",
    "    return text2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#https://newsapi.org/docs\n",
    "response = requests.get(\"https://newsapi.org/v2/top-headlines?sources=bbc-news&apiKey=1f83e742c8804de0a5d427510829f79b\")\n",
    "\n",
    "list_urls = []\n",
    "for i in range(0,len(response.json()['articles'])):\n",
    "    list_urls.append(response.json()['articles'][i]['url'])  \n",
    "    \n",
    "list_texts = []\n",
    "for i in range(0,len(list_urls)):\n",
    "    list_texts.append(get_text_from_url_bbc(list_urls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text = pd.DataFrame(list_texts, columns=['text'])\n",
    "data_texts_text['len'] = data_texts_text.text.apply(lambda x: len(x))\n",
    "data_texts_text['real'] = 'True'\n",
    "data_texts_text = data_texts_text[data_texts_text.len>0]\n",
    "clean_texts_test = data_texts_text.text.progress_apply(lambda x: clean_review(x, tokenizer, englishStemmer, stopwords_en_stem))\n",
    "\n",
    "X_test_sparse_texts = count_vectorizer.transform(clean_texts_test)\n",
    "X_test_texts = pd.DataFrame(X_test_sparse_texts.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "X_test_svd_texts = svd.transform(X_test_texts)\n",
    "X_test_bigram_sparse_texts = count_vectorizer_bigram.transform(clean_texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text['predict_reg_base'] = reg_model.predict(X_test_texts)\n",
    "data_texts_text['predict_prob_reg_base'] = reg_model.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_svd'] = reg_model_svd.predict(X_test_svd_texts)\n",
    "data_texts_text['predict_prob_svd'] = reg_model_svd.predict_proba(X_test_svd_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_bigram'] = reg_model_bigram.predict(X_test_bigram_sparse_texts)\n",
    "data_texts_text['predict_prob_bigram'] = reg_model_bigram.predict_proba(X_test_bigram_sparse_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_my_tree'] = my_tree.predict(X_test_texts)\n",
    "data_texts_text['predict_prob_my_tree'] = my_tree.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_explainer = np.array(X_train)\n",
    "explainer = LimeTabularExplainer(X_train_explainer, \n",
    "                                 mode = \"classification\",\n",
    "                                 training_labels = y_train,\n",
    "                                 feature_names = X_train.columns,\n",
    "                                 discretize_continuous=False)\n",
    "\n",
    "#i = 13\n",
    "#data_row = np.array(X_test.iloc[i])\n",
    "data_row = np.array(X_test_texts.iloc[4])\n",
    "explanation = explainer.explain_instance(data_row, reg_model.predict_proba, num_features=10)\n",
    "explanation.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca96728",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text.text.iloc[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dhdsblend2021] *",
   "language": "python",
   "name": "conda-env-dhdsblend2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
