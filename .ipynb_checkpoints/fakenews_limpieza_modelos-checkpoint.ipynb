{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60b6ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium\n",
    "#!pip install webdriver-manager\n",
    "#!pip install bs4\n",
    "#!pip install xgboost\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import requests\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb6865ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  real  \n",
       "0  December 31, 2017  Fake  \n",
       "1  December 31, 2017  Fake  \n",
       "2  December 30, 2017  Fake  \n",
       "3  December 29, 2017  Fake  \n",
       "4  December 25, 2017  Fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names_fake = pd.read_csv('./data/Fake.csv')\n",
    "file_names_fake['real'] = 'Fake'\n",
    "file_names_true = pd.read_csv('./data/True.csv')\n",
    "file_names_true['real'] = 'True'\n",
    "file_names = file_names_fake.append(file_names_true, ignore_index=True)\n",
    "file_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffd9974",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6e92c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(file_names, test_size=0.5, random_state=25)\n",
    "sample_size = 500\n",
    "data_train = data_train.sample(sample_size)\n",
    "data_test = data_test.sample(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed12241",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask_pos = np.array(Image.open(path.join(d, \"Thumbs.png\")))\n",
    "mask_neg = np.array(Image.open(path.join(d, \"Thumbsdw.png\")))\n",
    "\n",
    "wordcloud_fake = WordCloud(stopwords=stopwords_en,  mask=mask_neg, max_words=500, background_color=\"white\").generate(data_train[data_train['real']=='Fake'].text.str.cat(sep=' '))\n",
    "wordcloud_true = WordCloud(stopwords=stopwords_en, mask=mask_pos, max_words=500, background_color=\"white\").generate(data_train[data_train['real']=='True'].text.str.cat(sep=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    blues = \"hsl(215,100%%, %d%%)\" % np.random.choice(np.arange(25, 65, 5, dtype=int))\n",
    "    reds = \"hsl(0,100%%, %d%%)\" % np.random.choice(np.arange(40, 80, 5, dtype=int))\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 0:\n",
    "        return blues\n",
    "    else:\n",
    "        return reds\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20,10))  \n",
    "\n",
    "axs[1].imshow(wordcloud_fake.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].title.set_text('FAKE')\n",
    "axs[0].imshow(wordcloud_true.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].title.set_text('TRUE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccde534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_stopword_filtering(stopwords_en): \n",
    "    stopwords_en.append('reuter')\n",
    "    stopwords_en.append('said')\n",
    "    stopwords_en.append('Reuters')\n",
    "    stopwords_en.append('via')\n",
    "    stopwords_en.append('imag')\n",
    "    stopwords_en.append('https')\n",
    "    stopwords_en.append('com')\n",
    "    stopwords_en.append('one')\n",
    "    stopwords_en.append('u')\n",
    "    stopwords_en.append('also')\n",
    "    stopwords_en.append('would')\n",
    "    stopwords_en.append('featur')\n",
    "    stopwords_en.append('pic')\n",
    "    stopwords_en.append('us')\n",
    "    stopwords_en.append('wednesday')\n",
    "    stopwords_en.append('friday')\n",
    "    stopwords_en.append('monday')\n",
    "    stopwords_en.append('tuesday')\n",
    "    stopwords_en.append('saturday')\n",
    "    stopwords_en.append('sunday')\n",
    "    stopwords_en.append('thursday')\n",
    "    stopwords_en.append('getti')\n",
    "    stopwords_en.append('read')\n",
    "    stopwords_en.append('gop')\n",
    "    stopwords_en.append('watch')\n",
    "    stopwords_en.append('donald')\n",
    "    stopwords_en.append('trump')\n",
    "    stopwords_en.append('hillari')\n",
    "    stopwords_en.append('mr')\n",
    "    stopwords_en.append('accord')\n",
    "    stopwords_en.append('america')\n",
    "    stopwords_en.append('seem')\n",
    "    stopwords_en.append('youtub')\n",
    "    stopwords_en.append('21st')\n",
    "    stopwords_en.append('2017')\n",
    "    stopwords_en.append('www')\n",
    "    return stopwords_en\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en_filtered = manual_stopword_filtering(stopwords_en)\n",
    "porter = PorterStemmer()\n",
    "stopwords_en_porter = [porter.stem(x) for x in stopwords_en]\n",
    "stopwords_en_filtered_poter = [porter.stem(x) for x in stopwords_en_filtered]\n",
    "def porter_tokenizer(text):\n",
    "    ''' Esta función se utiliza cuando se sobre escribe el proceso de tokenización para que se pueda \n",
    "    obtener también la raíz de la palabra.'''\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words]  # remover los caracteres especiales\n",
    "    words = [word.replace(' ', '') for word in words] #remover los espacios en blanco\n",
    "    words = [word for word in words if word != '']\n",
    "    words = [PorterStemmer().stem(word) for word in words] # obtener la raiz de las palabras\n",
    "    words = [word for word in words if word not in stopwords_en_porter] # remover las stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer= CountVectorizer(lowercase = True, strip_accents='unicode', \\\n",
    "                           tokenizer = porter_tokenizer, ngram_range = (1, 3), min_df = 3)\n",
    "count_vectorizer.fit(data_train['text'])\n",
    "\n",
    "X_train_sparse = count_vectorizer.transform(data_train['text'])\n",
    "X_test_sparse = count_vectorizer.transform(data_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0440e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidft = TfidfTransformer()\n",
    "tfidft.fit(X_train_sparse)\n",
    "\n",
    "X_train = tfidft.transform(X_train_sparse)\n",
    "X_test = tfidft.transform(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data_train.real\n",
    "y_test = data_test.real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ce672",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b16f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "NB_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "myColors = ((0.90, 0.96, 1, 1), (0.70, 0.87, 1, 1), (0, 0.40, 0.75, 0.88))\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', myColors,10)\n",
    "axis_labels = np.sort(y_test.unique())\n",
    "cm_nb = confusion_matrix(y_test, NB_model.predict(X_test))\n",
    "sns.heatmap(cm_nb, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc285b",
   "metadata": {},
   "source": [
    "#### Modelo de Reg. Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(C = 7, solver=\"saga\", l1_ratio=0.5 , penalty=\"elasticnet\", n_jobs=-1)\n",
    "reg_model.fit(X_train, y_train)\n",
    "reg_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rlog = confusion_matrix(y_test, reg_model.predict(X_test))\n",
    "sns.heatmap(cm_rlog, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dense = pd.DataFrame(X_train.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "\n",
    "Coef_reg_model = pd.DataFrame(np.transpose(reg_model.coef_), index=X_train_dense.columns, columns=['Coef'])\n",
    "Coef_reg_model['coef_abs'] = abs(Coef_reg_model['Coef'])\n",
    "Coef_reg_model = Coef_reg_model.reset_index(-1)\n",
    "Coef_reg_model['words'] = Coef_reg_model['index'] \n",
    "Coef_reg_model['freq'] = (round(Coef_reg_model['coef_abs'] / Coef_reg_model.coef_abs.sum() * Coef_reg_model.shape[0] * 1000 , 0)).apply(lambda x: int(x))\n",
    "Coef_reg_model['odds_direction'] = Coef_reg_model['Coef'].apply(lambda x: 'Negative' if x<0 else 'Positive')\n",
    "Coef_reg_model_positive = Coef_reg_model[Coef_reg_model['odds_direction']=='Positive']\n",
    "Coef_reg_model_positive = Coef_reg_model_positive.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)\n",
    "Coef_reg_model_negative = Coef_reg_model[Coef_reg_model['odds_direction']=='Negative']\n",
    "Coef_reg_model_negative = Coef_reg_model_negative.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_pos = Coef_reg_model_positive.set_index('words').T.to_dict('index')\n",
    "tuples_neg = Coef_reg_model_negative.set_index('words').T.to_dict('index')\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask_pos = np.array(Image.open(path.join(d, \"Thumbs.png\")))\n",
    "mask_neg = np.array(Image.open(path.join(d, \"Thumbsdw.png\")))\n",
    "\n",
    "wordcloud_positive = WordCloud(max_words=500, mask=mask_pos, background_color=\"white\").generate_from_frequencies(dict(tuples_pos['freq']))\n",
    "wordcloud_negative = WordCloud(max_words=500, mask=mask_neg,background_color=\"white\").generate_from_frequencies(dict(tuples_neg['freq']))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2,figsize=(20,10))\n",
    "# Display the generated image:\n",
    "axs[0].imshow(wordcloud_positive.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].title.set_text('Positive')\n",
    "axs[1].imshow(wordcloud_negative.recolor(color_func=blue_color_func, random_state=3),interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].title.set_text('Negative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533a50",
   "metadata": {},
   "source": [
    "## Modelo de Reg. Logistica Truncando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components = 300);\n",
    "\n",
    "X_train_svd = svd.fit_transform(X_train)\n",
    "X_test_svd = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451615b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(svd.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model_svd = LogisticRegression(C = 8, solver=\"newton-cg\", penalty=\"l2\")\n",
    "reg_model_svd.fit(X_train_svd, y_train)\n",
    "reg_model_svd.score(X_test_svd, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rlog_svd = confusion_matrix(y_test, reg_model_svd.predict(X_test_svd))\n",
    "sns.heatmap(cm_rlog_svd, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acace5",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "my_tree.fit(X_train, y_train)\n",
    "my_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tree = confusion_matrix(y_test, my_tree.predict(X_test))\n",
    "sns.heatmap(cm_tree, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea932f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "tree.plot_tree(my_tree,feature_names = X_train_dense.columns,filled=True,rounded=True, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675f193",
   "metadata": {},
   "source": [
    "## Boosting de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc195ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = ExtraTreesClassifier(n_estimators=100, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n",
    "\n",
    "boost_tree = AdaBoostClassifier(base_estimator = base_classifier, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8,                                       \n",
    "                            random_state = 127)\n",
    "\n",
    "boost_tree.fit(X_train, y_train) \n",
    "boost_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_boost_tree = confusion_matrix(y_test, boost_tree.predict(X_test))\n",
    "sns.heatmap(cm_boost_tree, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6eb9b",
   "metadata": {},
   "source": [
    "## A traves de webscraping se buscan noticias para obtener más datos de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=chrome_options)\n",
    "\n",
    "def get_text_from_url_bbc(url):\n",
    "    wd.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(wd.page_source)\n",
    "    text = soup.find_all('div', {'data-component': 'text-block'}) #(?P<text>\\>(.*?)\\<)\n",
    "    text = reversed(text) \n",
    "    text2 = \"\"\n",
    "    for i in text:\n",
    "        try:\n",
    "            text2 = i.text+text2\n",
    "        except:\n",
    "            text2 = \"\"+text2\n",
    "    return text2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#https://newsapi.org/docs\n",
    "response = requests.get(\"https://newsapi.org/v2/top-headlines?sources=bbc-news&apiKey=1f83e742c8804de0a5d427510829f79b\")\n",
    "\n",
    "list_urls = []\n",
    "for i in range(0,len(response.json()['articles'])):\n",
    "    list_urls.append(response.json()['articles'][i]['url'])  \n",
    "    \n",
    "list_texts = []\n",
    "for i in range(0,len(list_urls)):\n",
    "    list_texts.append(get_text_from_url_bbc(list_urls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a9f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text = pd.DataFrame(list_texts, columns=['text'])\n",
    "data_texts_text['len'] = data_texts_text.text.apply(lambda x: len(x))\n",
    "data_texts_text['real'] = 'True'\n",
    "data_texts_text = data_texts_text[data_texts_text.len>0]\n",
    "\n",
    "X_test_sparse_texts = count_vectorizer.transform(data_texts_text.text)\n",
    "X_test_sparse_texts = tfidft.transform(X_test_sparse_texts)\n",
    "X_test_texts = pd.DataFrame(X_test_sparse_texts.todense(), \n",
    "             columns = count_vectorizer.get_feature_names()) \n",
    "X_test_svd_texts = svd.transform(X_test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text['predict_reg_base'] = reg_model.predict(X_test_texts)\n",
    "data_texts_text['prob_reg_base'] = reg_model.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_svd'] = reg_model_svd.predict(X_test_svd_texts)\n",
    "data_texts_text['prob_svd'] = reg_model_svd.predict_proba(X_test_svd_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_my_tree'] = my_tree.predict(X_test_texts)\n",
    "data_texts_text['prob_my_tree'] = my_tree.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_boost_tree'] = boost_tree.predict(X_test_texts)\n",
    "data_texts_text['prob_boost_tree'] = boost_tree.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_text_style = data_texts_text.style.format({\"prob_reg_base\": \"{:.2%}\", \n",
    "                          \"prob_svd\": \"{:.2%}\", \n",
    "                          \"prob_my_tree\": \"{:.2%}\",\n",
    "                          \"prob_boost_tree\":\"{:.2%}\"})\\\n",
    "                 .format({\"text\": lambda x:x[:50]+\"...\"})\\\n",
    "                 .hide_index()\\\n",
    "                .format({\"len\": lambda x: \"{:.0f}\".format(x)})\n",
    "\n",
    "data_text_style = data_text_style.set_properties(**{'background-color': 'white',\n",
    "                           'color': 'grey',\n",
    "                           'border-color': 'black'})\n",
    "data_text_style.applymap(lambda x: 'color: red' if (x)=='Fake' else '')\n",
    "data_text_style.set_table_styles(\n",
    "   [{\n",
    "       'selector': 'th',\n",
    "       'props': [('background-color', '#006AFF')]\n",
    "   }])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
