{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a31fb20",
   "metadata": {},
   "source": [
    "# Trabjo Práctico Integrador: Grupo 8\n",
    "## Integrantes:\n",
    "\n",
    "- Choconi Lucas\n",
    "- Berra Eliel\n",
    "- Mina Federico\n",
    "- Scillato German"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450bee1",
   "metadata": {},
   "source": [
    "<a id=\"section_toc\"></a> \n",
    "## INDICE\n",
    "\n",
    "[Importaciones de librerías](#section_Import)\n",
    "\n",
    "[Configuración de gráficos](#section_config)\n",
    "\n",
    "[Flujo de construcción del Modelo Predictor](#section_logica)\n",
    "\n",
    "[Análisis y Limpieza de datos](#section_limpieza)\n",
    "\n",
    "[Feature Selection](#section_feature)\n",
    "\n",
    "[Pre-Procesamiento](#section_prepros)\n",
    "\n",
    "$\\hspace{.5cm}$[Cambio de Base](#section_cambio)\n",
    "\n",
    "$\\hspace{.5cm}$[Aplicacion SVD](#section_cambio_svd)\n",
    "\n",
    "[Modelos](#section_modelos)\n",
    "\n",
    "$\\hspace{.5cm}$[Naive Bayes](#section_naive)\n",
    "\n",
    "$\\hspace{.5cm}$[Regresión Logística](#section_rl)\n",
    "\n",
    "$\\hspace{.8cm}$[Regresión Logística Truncando palabras](#section_rl_trun)\n",
    "\n",
    "$\\hspace{.5cm}$[Decision Tree Classifier](#section_tree)\n",
    "\n",
    "$\\hspace{.5cm}$[Boosting de Árboles](#section_boost)\n",
    "\n",
    "[Evaluación de Modelos - ROC curve](#section_eval)\n",
    "\n",
    "[Pipeline](#section_pipe)\n",
    "\n",
    "[Exportación del Modelo Entrenado](#section_pickle)\n",
    "\n",
    "[Web App](#section_webapp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f532a",
   "metadata": {},
   "source": [
    "<a id=\"section_Import\"></a>  \n",
    "## Importaciones de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba54bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "\n",
    "from joblib import parallel_backend\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7100ab",
   "metadata": {},
   "source": [
    "<a id=\"section_config\"></a>  \n",
    "## Configuración de gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caabd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración Graficos \n",
    "myColors = ((0.90, 0.96, 1, 1), (0.70, 0.87, 1, 1), (0, 0.40, 0.75, 0.88))\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', myColors,10)\n",
    "\n",
    "# Fuente de titulos de gráficos\n",
    "font = {'color':  'darkred', 'weight': 'normal', 'size': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378fd10",
   "metadata": {},
   "source": [
    "<a id=\"section_logica\"></a>  \n",
    "\n",
    "## Flujo de construcción del Modelo Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797cd74",
   "metadata": {},
   "source": [
    "![Diagrama](./data/fake_d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1136dcd",
   "metadata": {},
   "source": [
    "<a id=\"section_limpieza\"></a>\n",
    "\n",
    "## Limpieza de datos  y Analisis Exploratorio del Dataset\n",
    "Importación y limpieza del \n",
    "[Dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24083b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# El dataset vino separado en las noticias Falsas y Verdaderas, así que se lo unifica con append \n",
    "# entre los archivos True.csv y Fake.csv\n",
    "\n",
    "file_names_true = pd.read_csv('./data/True.csv')\n",
    "file_names_true['real'] = 'True'\n",
    "file_names_fake = pd.read_csv('./data/Fake.csv')\n",
    "file_names_fake['real'] = 'Fake'\n",
    "data = file_names_fake.append(file_names_true, ignore_index=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'El dataset tiene un tamaño de {data.shape[0]} registros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas filas contienen datos erróneos que impiden transformar la columna 'date' a formato fecha\n",
    "wrong_row = data['date'] == 'https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/'\n",
    "data[wrong_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las filas que no pueden ser correctamente transformadas a datetime\n",
    "# serán utilizadas en una máscara booleana para poder eliminarlas\n",
    "\n",
    "data['date'] = data['date'].apply(lambda x: pd.to_datetime(x,\n",
    "                                                    infer_datetime_format = True)\\\n",
    "                                                    if re.search('[a-z-A-Z]+ [0-9]+, [0-9]{4}', x) else  np.NaN)\n",
    "delete_mask = data['date'].notna()\n",
    "data = data[delete_mask]\n",
    "data = data.reset_index()\n",
    "data['date'] = data['date'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "ax =sns.histplot(data = data, x = 'subject', hue = 'real'  )\n",
    "plt.ylabel('Cantidad de noticias')\n",
    "plt.xlabel('Categoría')\n",
    "plt.title(\"Categoría de las Noticias\", fontdict= font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como hay valores diferentes en la columna \"subject\" para las noticias reales y las falsas borramos esa columna\n",
    "data = data.drop(columns = 'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c55e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separamos los datos en dos máscaras con las filas que contienen noticias falsas de las que son reales\n",
    "mask_real = data['real'] == 'True'\n",
    "mask_fake = data['real'] == 'Fake'\n",
    "\n",
    "data_real = data[mask_real]\n",
    "data_fake = data[mask_fake]\n",
    "\n",
    "# Hacemos un gráfico mostrando el conteo de cuántas noticias falsas y cuántas verdaderas se publicaron \n",
    "# por mes en el dataset \n",
    "plt.figure(figsize=(12,6))\n",
    "ax = data_real['date'].groupby([data_real[\"date\"].dt.year,\n",
    "                             data_real[\"date\"].dt.month]).count().plot.line(label = 'Real news')\n",
    "\n",
    "ax = data_fake['date'].groupby([data_fake[\"date\"].dt.year,\n",
    "                            data_fake[\"date\"].dt.month]).count().plot.line(label = 'Fake news')\n",
    "ax.set(xlabel=None)\n",
    "ax.set_title('Linea Temporal Noticias' , fontdict = font)\n",
    "ax.set_xlabel('Año / Mes')\n",
    "ax.set_ylabel('Cantidad Noticias')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a990792",
   "metadata": {},
   "source": [
    "El pico de noticias verdaderas publicadas en Noviembre del 2016 coincide con las elecciones presidenciales de Estados Unidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b85b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se observa que el dataset está muy balanceado. Cuando se evalúe el accuracy de un modelo deberá\n",
    "# compararse contra el %50 de la estratificación de la variable target.\n",
    "\n",
    "ax = sns.countplot(data['real'])\n",
    "ax.set_title('Análisis Balanceo del dataset', fontdict = font)\n",
    "ax.set_ylabel(\"Cantidad\")\n",
    "ax.set_xlabel(\"Noticias Falsas y Verdades\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b475a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las dos siguientes operaciones suelen demorar mucho en ejecutarse, por eso se va a trabajar con un sampleo\n",
    "# de los datos, intentando aumentar la velocidad y que su ejecución se más práctica. Si no se trabaja\n",
    "# con el sampleo, la función que se encuentra más abajo que obtiene el porcentaje de palabras del artículo\n",
    "# que están en el diccionario se demoraría 40hs, con el sampleo se reduce a 1 hora.\n",
    "\n",
    "data_sample = data.sample(1000)\n",
    "mask_real = data_sample['real'] == 'True'\n",
    "mask_fake = data_sample['real'] == 'Fake'\n",
    "data_real_sample = data_sample[mask_real]\n",
    "data_fake_sample = data_sample[mask_fake]\n",
    "\n",
    "# Averiguamos cuál es el largo promedio de las noticias falsas y de las reales\n",
    "\n",
    "tqdm.pandas()                        \n",
    "data_real_sample['text_len'] = data_real_sample['text'].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "data_fake_sample['text_len'] = data_fake_sample['text'].progress_apply(lambda x: len(word_tokenize(x)))\n",
    "\n",
    "# Gráfico comparativo del largo promedio de las noticias falsas y verdaderas\n",
    "real_len_avg = data_real_sample['text_len'].mean()\n",
    "fake_len_avg = data_fake_sample['text_len'].mean()\n",
    "len_avgs = [real_len_avg, fake_len_avg]\n",
    "plt.bar(['Real', 'Fake'], len_avgs, color = ['blue', 'orange'])\n",
    "plt.title('Promedio de cantidad de palabras por noticia' , fontdict= font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5809428",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Lista con todas las palabras en inglés https://github.com/dwyl/english-words/blob/master/words.txt\n",
    "all_eng_words = pd.read_csv('./data/all_english_words.csv')\n",
    "all_eng_words = pd.Series(all_eng_words.iloc[:,0])\n",
    "all_eng_words = all_eng_words.apply(lambda x: x.lower() if type(x) == str else x)\n",
    "\n",
    "def words_percentage(text):\n",
    "    #Devuelve el porcentaje de palabras que posee un texto que están dentro de un diccionario\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words] \n",
    "    words_in_dict = [word for word in words if word in all_eng_words.values]\n",
    "    try: \n",
    "        words_in_dict_percentage = len(words_in_dict) * 100 / len(words)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "    return words_in_dict_percentage\n",
    "\n",
    "data_sample['words_in_dict_pct'] = data_sample['text'].progress_apply(words_percentage)\n",
    "\n",
    "data_sample_pct_location = './data/data_with_pct.csv'\n",
    "data_sample.to_csv(data_sample_pct_location, index = False)\n",
    "'''\n",
    "''' Esta celda demora apróximadamente 50 minutos en ejecutarse. Se puede obtener el dataset con los resultados\n",
    "de esta operación ejecutando la celda de abajo.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levantamos el dataset de la celda de arriba\n",
    "data_sample_pct_location = './data/data_with_pct.csv'\n",
    "data_sample = pd.read_csv(data_sample_pct_location)\n",
    "mask_real = data_sample['real'] == 'True'\n",
    "mask_fake = data_sample['real'] == 'Fake'\n",
    "data_real_sample = data_sample[mask_real]\n",
    "data_fake_sample = data_sample[mask_fake]\n",
    "\n",
    "# El objetivo es identificar si las noticias verdaderas utilizan más palabras reconocidas por el diccionario\n",
    "# que las noticias falsas, que probablemente utilicen más el lenguaje conocido como \"slang\"\n",
    "# Para verificar eso se realiza el siguiente gráfico\n",
    "real_words_dict_pct = data_real_sample['words_in_dict_pct'].mean()\n",
    "fake_words_dict_pct = data_fake_sample['words_in_dict_pct'].mean()\n",
    "words_dict_pcts = [real_words_dict_pct, fake_words_dict_pct]\n",
    "plt.bar(['Real', 'Fake'], words_dict_pcts, color = ['blue', 'orange'])\n",
    "plt.title('Porcentaje Promedio  de palabras por noticia presentes en diccionario' , fontdict= font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663160e",
   "metadata": {},
   "source": [
    "Nuestra hipótesis de que las noticias verdaderas iban a contenter un promedio del porcentaje de palabras contenidas en el diccionario no era correcta, es un poco mayor pero la diferencia es despreciable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed12241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar las manos con pulgar hacia arriba/abajo\n",
    "\n",
    "def blue_color_func(word, font_size, position,orientation, **kwargs):\n",
    "    \"\"\"Argumentos de función necesarias para metodo worldcloud\"\"\"\n",
    "    blues = \"hsl(215,100%%, %d%%)\" % np.random.choice(np.arange(25, 65, 5, dtype=int))\n",
    "    reds = \"hsl(0,100%%, %d%%)\" % np.random.choice(np.arange(40, 80, 5, dtype=int))\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 0:\n",
    "        return blues\n",
    "    else:\n",
    "        return reds\n",
    "\n",
    "def graficos_manos(fake,true):\n",
    "    \"\"\"Función para graficar worldcloud en forma de manos,\n",
    "    falsas y verdaderas. Variables: worldcloud falsa y verdadera\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(20,10))  \n",
    "\n",
    "    axs[1].imshow(fake.recolor(color_func=blue_color_func, random_state=3),\n",
    "                interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[1].set_title('FAKE' , fontdict = font)\n",
    "    axs[0].imshow(true.recolor(color_func=blue_color_func,random_state=3),\n",
    "                interpolation=\"bilinear\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[0].set_title('TRUE' , fontdict = font)\n",
    "    plt.show()\n",
    "    \n",
    "# Importación de imagenes para graficos de manos.\n",
    "\n",
    "mask_pos = np.array(Image.open(\"./data/Thumbs.png\"))\n",
    "mask_neg = np.array(Image.open(\"./data/Thumbsdw.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb92bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un gráfico que muestra visualmente las palabras que más frecuentemente aparecen tanto en noticias\n",
    "# falsas como verdaderas\n",
    "\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "wordcloud_fake = WordCloud(stopwords=stopwords_en,\n",
    "            mask=mask_neg, \n",
    "            max_words=500,\n",
    "            background_color=\"white\").generate(data_real.text.str.cat(sep=' '))\n",
    "wordcloud_true = WordCloud(stopwords=stopwords_en,\n",
    "            mask=mask_pos,\n",
    "            max_words=500,\n",
    "            background_color=\"white\").generate(data_fake.text.str.cat(sep=' '))\n",
    "\n",
    "graficos_manos(fake = wordcloud_fake,true = wordcloud_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b18b54",
   "metadata": {},
   "source": [
    "Se verifica que el dataset no posea NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El dataset no valores nulos así que damos por finalizada la limpieza y el Análisis Exploratorio Inicial\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a001533",
   "metadata": {},
   "source": [
    "<a id=\"section_sinstop\"></a>  \n",
    "## Feature Selection \n",
    "\n",
    "### Removiendo stopwords que overfittean el modelo\n",
    "En esta sección se analiza el sobreajuste para el dataset inicial, evauando las palabras que están overfiteando el modelo, es decir, aquellas que de forma arbitraria elevan el score del modelo pero que hace que pierda capacidad predicitiva generalizada. Para ello se utiliza un modelo de ábroles y a través de la propiedad de feature importance se observa el puntaje de las palabras que poseen un coeficiente muy elevado.\n",
    "\n",
    "Se consideran los siguientes puntos para esta sección\n",
    "- Se utiliza un modelo simple de tree, con tendencia a sobreajuste.\n",
    "- En las secciones siguientes se vuelve a definir los X_train, X_test, y_train e y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se va a utilizar el dataset completo para evaluar todas las stopwords posibles que estén sobre ajustando\n",
    "# el modelo\n",
    "X = data['text']\n",
    "y = data['real']\n",
    "X_train_completo, X_test_completo, y_train_completo, y_test_completo = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd5e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de función para el tokenizer\n",
    "def porter_tokenizer(text):\n",
    "    ''' Esta función se utiliza cuando se sobre escribe el proceso\n",
    "    de tokenización en la clase CountVectorizer para que se pueda\n",
    "     obtener también la raíz de la palabra con el método Porter.'''\n",
    "    porter = PorterStemmer()\n",
    "    # Tener en cuenta que está utilizando la variable global stopwords_en\n",
    "    stopwords_en_porter = [porter.stem(x) for x in stopwords_en]\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words]  # remover los caracteres especiales\n",
    "    words = [word.replace(' ', '') for word in words] #remover los espacios en blanco\n",
    "    words = [word for word in words if word != '']\n",
    "    words = [porter.stem(word) for word in words] # obtener la raiz de las palabras\n",
    "    words = [word for word in words if word not in stopwords_en_porter] # remover las stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de función para el tokenizer con Lancaster Stemmer que será utilizado en el pipeline\n",
    "def lancaster_tokenizer(text):\n",
    "    ''' Esta función se utiliza cuando se sobre escribe el proceso\n",
    "    de tokenización en la clase CountVectorizer para que se pueda\n",
    "     obtener también la raíz de la palabra con el ḿetodo Lancaster.'''\n",
    "    lancaster = LancasterStemmer()\n",
    "    # Tener en cuenta que está utilizando la variable global stopwords_en\n",
    "    stopwords_en_lancaster = [lancaster.stem(x) for x in stopwords_en]\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words]  # remover los caracteres especiales\n",
    "    words = [word.replace(' ', '') for word in words] #remover los espacios en blanco\n",
    "    words = [word for word in words if word != '']\n",
    "    words = [lancaster.stem(word) for word in words] # obtener la raiz de las palabras\n",
    "    words = [word for word in words if word not in stopwords_en_lancaster] # remover las stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700705a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define stopwords y se genera nuevamente la vectorización, y la matriz tfidf\n",
    "\n",
    "customized_vectorizer_feature= CountVectorizer(lowercase = True,\n",
    "                                               strip_accents='unicode', \n",
    "                                               tokenizer = porter_tokenizer,\n",
    "                                               ngram_range = (1, 3),\n",
    "                                               min_df = 3)\n",
    "\n",
    "customized_vectorizer_matrix = customized_vectorizer_feature.fit_transform(X_train_completo)\n",
    "customized_vectorizer_test = customized_vectorizer_feature.transform(X_test_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09826d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_train_completo = TfidfTransformer().fit_transform(customized_vectorizer_matrix)\n",
    "tfidf_matrix_test_completo = TfidfTransformer().fit_transform(customized_vectorizer_test) \n",
    "\n",
    "X_train_completo= tfidf_matrix_train_completo\n",
    "X_test_completo = tfidf_matrix_test_completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47be6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo ejemplo para mostrar sobre ajuste.\n",
    "my_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "my_tree.fit(X_train_completo , y_train_completo )\n",
    "print(\"Score modelo con stopwords sin retirar: {} %\".format(my_tree.score(X_test_completo ,\n",
    "                                            y_test_completo )*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b340c",
   "metadata": {},
   "source": [
    "Se observa un accuracy muy elevado, cercano al 1, lo que indica un claro sobre ajuste. Si se quiere comparar el score entre este árbol overfitteado y un árbol sin las palabras que sobreajustan el modelo ir a la sección [Decision Tree Classifier](#section_tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se evalúa el  peso de cada palabra.\n",
    "importancia_features = pd.DataFrame(my_tree.feature_importances_,\n",
    "                                 index = customized_vectorizer_feature.get_feature_names(),\n",
    "                                 columns=['importancia'])\n",
    "importancia_features_sort = importancia_features.sort_values('importancia', ascending=False)\n",
    "print(importancia_features_sort[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18368d",
   "metadata": {},
   "source": [
    "En la celda de arriba se observa que la palabra \"reuter\" está overfiteando el modelo, ya que muchas de las noticias verdaderas contienen la palabra \"Reuters\" en su contenido, siendo este el medio que las emite. \n",
    "Por eso se la va a añadir como stopword. Este procedimiento se iteró forma manual y repetida, identificando en cada corrida de cada modelo cuáles eran las palabras que overfiteaban el modelo, para luego añadirlas al listado de stopwords para agregar que figura en la celda de abajo. No se agregan todas las pruebas realizadas para no llenar de código innecesario la notebook, pero se deja constancia y explicación del procedimiento realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregado de Stopwords específicas del dataset que overfittean el modelo.\n",
    "\n",
    "def append_stopword(listado_stopwords, stopwords_agregar):\n",
    "    for stopwords in stopwords_agregar:\n",
    "        listado_stopwords.append(stopwords)\n",
    "    return listado_stopwords\n",
    "\n",
    "stopwords_agregar = ['reuter','said','Reuters','via','imag','https','com','one',\n",
    "'u','also','would','featur','pic','us','wednesday','friday','monday','tuesday',\n",
    "'saturday','sunday','thursday','getti','read','gop','watch','donald','trump',\n",
    "'hillari','mr','accord','america','seem','youtub','21st',\n",
    "'video' , 'http' , 'like' , 'obama' , 'minist' , 'washington' , 'know' , 'wwwyoutubecomwatch' ]\n",
    "\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stopwords_en_porter = [porter.stem(x) for x in stopwords_en]\n",
    "stopwords_en = append_stopword(stopwords_en_porter ,stopwords_agregar )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4665abb",
   "metadata": {},
   "source": [
    "<a id=\"section_prepros\"></a>  \n",
    "\n",
    "## Pre-procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a361cf3",
   "metadata": {},
   "source": [
    "Se divide el dataset en train y test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas operaciones se demoran mucho tiempo debido a la gran cantidad de columnas que se generan al vectorizar\n",
    "# los datos, el tamaño pequeño de la muestra es para agilizar las operaciones. De todas maneras, el tamaño\n",
    "# de la muestra es lo suficientemente significativo como obtener un buen accuracy.\n",
    "sample_size = 1000 \n",
    "testing_sample_size = 50 # Para hacer testeos aún más rápidos se recomienda este sampleo \n",
    "data_sample = data.sample(sample_size)\n",
    "X = data_sample['text']\n",
    "y = data_sample['real']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa3faf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# La ventaja de este CountVectorizer es que incluye una función porter_tokenizer como parámetro que permite\n",
    "# customizar el procedimiento para obtener la raíz de la palabra\n",
    "customized_vectorizer= CountVectorizer(lowercase = True,\n",
    "                                       strip_accents='unicode', \n",
    "                                       tokenizer = porter_tokenizer,\n",
    "                                       ngram_range = (1, 3),\n",
    "                                       min_df = 3)\n",
    "\n",
    "customized_vectorizer_train = customized_vectorizer.fit_transform(X_train)\n",
    "customized_vectorizer_test = customized_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f863532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construcción de matrices tfidf\n",
    "tfidf_matrix_train = TfidfTransformer().fit_transform(customized_vectorizer_train)\n",
    "tfidf_matrix_test = TfidfTransformer().fit_transform(customized_vectorizer_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab57ee0",
   "metadata": {},
   "source": [
    "<a id=\"section_cambio\"></a>  \n",
    "### Cambio de Base\n",
    "\n",
    "Evaluación del impacto con distintas cantidades de componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_n_components(var_ratio, goal_var, should_graph = True): \n",
    "    ''' Esta función se utiliza para hacer un cambio de base y lograr explicar\n",
    "    el porcentaje de los datos que se pasa en el argumento goal_var.\n",
    "    Fuente https://chrisalbon.com/code/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    "    Fue modificada para que, sí el parámetro should_graph es True,haga un gráfico de la varianza explicativa\n",
    "    del modelo de acuerdo al número de componentes'''\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    # Listas del nuḿero de componentes y de la explicación de varianza alcanzada para poder graficarlas\n",
    "    num_components = list(range(1, len(var_ratio) + 1))\n",
    "    variances = []\n",
    "    # For the explained variance of each feature:\n",
    "    found_n_components = False\n",
    "    for explained_variance in var_ratio:\n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        # Agregar la varianza explicada al listado \n",
    "        variances.append(total_variance)\n",
    "        # If we reach our goal level of explained variance and we haven't reached our desired variance\n",
    "        if total_variance >= goal_var and found_n_components == False:\n",
    "            desired_n_components = n_components\n",
    "            found_n_components = True\n",
    "            desired_variance = total_variance\n",
    "    \n",
    "    if should_graph == True:\n",
    "        plt.plot(num_components, variances )\n",
    "        plt.scatter(desired_n_components, desired_variance, c = 'r')\n",
    "        plt.xlabel('Número de componentes')\n",
    "        plt.ylabel('Razón de la explicación de la varianza')\n",
    "        plt.hlines(desired_variance, 0, desired_n_components, 'r', 'dashed')\n",
    "        plt.vlines(desired_n_components, 0, desired_variance, 'r', 'dashed')\n",
    "        plt.title(\"Comparativa Varianza vs Número componentes \",fontdict=font)\n",
    "        \n",
    "    # Return the desired number of components. \n",
    "    return desired_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de cambio de base para 3 threshold\n",
    "# Evaluamos cómo responde el cambio de base cuando seleccionamos una explicación\n",
    "# de la varianza alta (0.9), media (0.5) o baja (0.1)\n",
    "desired_threshold = [0.9 , 0.5 , 0.1]\n",
    "\n",
    "# Tener en cuenta que si el número de filas de la matrix esparsa es menor al número de componentes elegido\n",
    "# como hiperparámetro, una vez que se haya realizado la transformación SVD el número de componentes \n",
    "# obtenido será igual al número de filas de la matriz esparsa, y no será el número de componentes\n",
    "# elegido como hiperpárametro\n",
    "for thres in desired_threshold:\n",
    "    test_svd = TruncatedSVD(n_components = tfidf_matrix_train.shape[1] - 1);\n",
    "    test_svd.fit(tfidf_matrix_train)\n",
    "    tsvd_var_ratios = test_svd.explained_variance_ratio_\n",
    "    desired_threshold = thres\n",
    "    n_components = select_n_components(tsvd_var_ratios, desired_threshold)\n",
    "\n",
    "    print(f'Para Threshold {thres} número de componentes después del cambio de base: {n_components}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ef46e",
   "metadata": {},
   "source": [
    "<a id=\"section_cambio_svd\"></a>  \n",
    "##### Aplicacion SVD\n",
    "\n",
    "Se concluye que se tiene una performance aceptable aunque solo se capture el 10% de la varianza del dataset. Por lo cual, en el pipeline se incluiran los 3 threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9afa2",
   "metadata": {},
   "source": [
    "<a id=\"section_modelos\"></a> \n",
    "## Modelos\n",
    "\n",
    "En esta sección se entrenan y testean distintos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de X train y test a partir de las matrices tfidf\n",
    "# Se utiliza la matriz esparsa sin transformarla a DataFrame para que los procesos se ejecuten más rápidamente\n",
    "# No utilizamos el cambio de base con SVD porque suponemos que esa pérdida de información impactará negativamente\n",
    "# en el accuracy, pero lo incluiremos en el Pipeline para verificarlo.\n",
    "\n",
    "X_train = tfidf_matrix_train\n",
    "X_test = tfidf_matrix_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ce672",
   "metadata": {},
   "source": [
    "<a id=\"section_naive\"></a> \n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b16f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "NB_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_labels = np.sort(y_test.unique())\n",
    "cm_nb = confusion_matrix(y_test, NB_model.predict(X_test))\n",
    "sns.heatmap(cm_nb, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales'), plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc285b",
   "metadata": {},
   "source": [
    "<a id=\"section_rl\"></a> \n",
    "### Regresión Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(C = 7, solver=\"saga\", l1_ratio=0.5 ,\n",
    "                                 penalty=\"elasticnet\", n_jobs=-1)\n",
    "reg_model.fit(X_train, y_train)\n",
    "reg_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rlog = confusion_matrix(y_test, reg_model.predict(X_test))\n",
    "sns.heatmap(cm_rlog, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "             cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales'), plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coef_reg_model = pd.DataFrame(np.transpose(reg_model.coef_), index=customized_vectorizer.get_feature_names(),\n",
    " columns=['Coef'])\n",
    "Coef_reg_model['coef_abs'] = abs(Coef_reg_model['Coef'])\n",
    "Coef_reg_model = Coef_reg_model.reset_index(-1)\n",
    "Coef_reg_model['words'] = Coef_reg_model['index'] \n",
    "Coef_reg_model['freq'] = (round(Coef_reg_model['coef_abs'] / Coef_reg_model.coef_abs.sum()* Coef_reg_model.shape[0] * 1000 , 0)).apply(lambda x: int(x))\n",
    "Coef_reg_model['odds_direction'] = Coef_reg_model['Coef'].apply(lambda x: 'Negative' if x<0 else 'Positive')\n",
    "\n",
    "Coef_reg_model_positive = Coef_reg_model[Coef_reg_model['odds_direction']=='Positive']\n",
    "Coef_reg_model_positive = Coef_reg_model_positive.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)\n",
    "Coef_reg_model_negative = Coef_reg_model[Coef_reg_model['odds_direction']=='Negative']\n",
    "Coef_reg_model_negative = Coef_reg_model_negative.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c29ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuples_pos = Coef_reg_model_positive.set_index('words').T.to_dict('index')\n",
    "tuples_neg = Coef_reg_model_negative.set_index('words').T.to_dict('index')\n",
    "\n",
    "wordcloud_positive = WordCloud(max_words=500, mask=mask_pos,\n",
    "        background_color=\"white\").generate_from_frequencies(dict(tuples_pos['freq']))\n",
    "wordcloud_negative = WordCloud(max_words=500, mask=mask_neg,\n",
    "        background_color=\"white\").generate_from_frequencies(dict(tuples_neg['freq']))\n",
    "\n",
    "graficos_manos(fake = wordcloud_negative,true = wordcloud_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533a50",
   "metadata": {},
   "source": [
    "<a id=\"section_rl_trun\"></a> \n",
    "#### Modelo de Reg. Logistica Truncando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svd = TruncatedSVD(n_components = X_train.shape[1] - 1);\n",
    "test_svd.fit(X_train)\n",
    "tsvd_var_ratios = test_svd.explained_variance_ratio_\n",
    "desired_tsh_10, desired_tsh_50, desired_tsh_90 = 0.1, 0.5, 0.9\n",
    "n_components_90_var = select_n_components(tsvd_var_ratios, desired_tsh_90, should_graph = False)\n",
    "n_components_50_var = select_n_components(tsvd_var_ratios, desired_tsh_50, should_graph = False)\n",
    "n_components_10_var = select_n_components(tsvd_var_ratios, desired_tsh_10, should_graph = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_90 = TruncatedSVD(n_components = n_components_90_var)\n",
    "svd_50 = TruncatedSVD(n_components = n_components_50_var)\n",
    "svd_10 = TruncatedSVD(n_components = n_components_10_var)\n",
    "\n",
    "X_train_svd_90 = svd_90.fit_transform(X_train)\n",
    "X_test_svd_90 = svd_90.transform(X_test)\n",
    "X_train_svd_50 = svd_50.fit_transform(X_train)\n",
    "X_test_svd_50 = svd_50.transform(X_test)\n",
    "X_train_svd_10 = svd_10.fit_transform(X_train)\n",
    "X_test_svd_10 = svd_10.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd = [[X_train_svd_90,X_test_svd_90],[X_train_svd_50,X_test_svd_50],[X_train_svd_10,X_test_svd_10]]\n",
    "scores = []\n",
    "models = []\n",
    "for svd in X_svd:\n",
    "    reg_model_svd = LogisticRegression(C = 8, solver=\"newton-cg\", penalty=\"l2\")\n",
    "    models.append(reg_model_svd.fit(svd[0], y_train))\n",
    "    scores.append(reg_model_svd.score(svd[1], y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, axs = plt.subplots(1,3, figsize= (15,5))\n",
    "title = [90,50,10]\n",
    "for i in range(3):\n",
    "    \n",
    "    cm_rlog_svd = confusion_matrix(y_test, models[i].predict(X_svd[i][1]))\n",
    "    sns.heatmap(cm_rlog_svd,xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "                cbar=False, annot=True, cmap=cmap, fmt='d', ax = axs[i],\n",
    "                linecolor = \"w\" , linewidths = .5,square=True)\n",
    "    axs[i].set_title(label = f'Tsh {title[i]} - Accuracy: {scores[i].round(2)} ' , \n",
    "    loc ='left', fontdict = font )\n",
    "    axs[0].set_ylabel('Etiquetas reales')\n",
    "    axs[i].set_xlabel('Etiquetas predichas')\n",
    "    plt.suptitle(t='SVD con diferentes Thresholds (Tsh)', fontsize = \"x-large\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acace5",
   "metadata": {},
   "source": [
    "<a id=\"section_tree\"></a> \n",
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
    "my_tree.fit(X_train, y_train)\n",
    "accuracy_tree = my_tree.score(X_test, y_test)\n",
    "print(accuracy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tree = confusion_matrix(y_test, my_tree.predict(X_test))\n",
    "sns.heatmap(cm_tree, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "            cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales'), plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea932f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "tree.plot_tree(my_tree,feature_names = customized_vectorizer.get_feature_names(),\n",
    "                filled=True,rounded=True, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675f193",
   "metadata": {},
   "source": [
    "<a id=\"section_boost\"></a> \n",
    "\n",
    "### Boosting de Árboles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc195ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_tree = AdaBoostClassifier(base_estimator = my_tree, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8)\n",
    "\n",
    "boost_tree.fit(X_train, y_train) \n",
    "boost_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_boost_tree = confusion_matrix(y_test, boost_tree.predict(X_test))\n",
    "sns.heatmap(cm_boost_tree, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales'), plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aedf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparación del accuracy de los modelos\n",
    "models = [NB_model, reg_model, my_tree, boost_tree]\n",
    "model_names = ['Multinomial Bayes', 'Logistic Regression', 'Decision Tree', 'AdaBoost']\n",
    "scores = [model.score(X_test, y_test) for model in models]\n",
    "\n",
    "plt.plot(model_names, scores)\n",
    "plt.title(\"Comparativa Accuracy Modelos\" , fontdict= font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe20d5",
   "metadata": {},
   "source": [
    "<a id=\"section_eval\"></a> \n",
    "\n",
    "## Evaluación de los modelos - Roc Curve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#   Se convierte etiquetas a Int 0 y 1\n",
    "y_roc = y_test.apply(lambda x: 1 if x == \"True\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fa610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(models, X_test , y_test):\n",
    "    \"Calcula las probabilidades y grafica la ROC Curve de los modelos en lista 'Modelos'. \"\n",
    "    for i, model in enumerate(models):\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        fpr_log, tpr_log, thr_log = roc_curve(y_roc, y_pred_proba[:,1])\n",
    "        df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "        plt.plot(df['fpr'],df['tpr'], label=model_names[i])\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.axis([0, 1.01, 0, 1.01])\n",
    "    \n",
    "    plt.xlabel('1 - Specificty'); plt.ylabel('TPR / Sensitivity'); plt.title('ROC Curve')\n",
    "    plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "ROC_curve(models = models, X_test = X_test , y_test = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96366c48",
   "metadata": {},
   "source": [
    "<a id=\"section_pipe\"></a> \n",
    "\n",
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccea44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las operaciones con grid search suelen tomar mucho tiempo, si se quiere hacer pruebas descomentar y\n",
    "# ejecutar esta celda para reducir el número de registros\n",
    "'''\n",
    "small_testing_sample_size = 20\n",
    "\n",
    "data_sample = data.sample(small_testing_sample_size)\n",
    "X = data_sample['text']\n",
    "y = data_sample['real']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "'''\n",
    "''''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se redefinen las variables features y target de entrenamiento y de testeo porque el X_train había sido\n",
    "# empleado como una Matriz esparsa de TFIDF, y en el pipeline se tienen que introducir los datos\n",
    "# sin que hayan pasado por el Pre Procesamiento. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasos = [('vectorizador', CountVectorizer()),\n",
    "         ('tfidf', TfidfTransformer()), \n",
    "         ('cambio_base', TruncatedSVD()),\n",
    "         ('classifier', LogisticRegression())]\n",
    "pipeline = Pipeline(pasos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fd96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase construida para generar cambios de base con diferente número de componentes en el pipeline\n",
    "class SVD_n_components(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y):\n",
    "        svd = TruncatedSVD(n_components = self.n_components);\n",
    "        return svd.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La búsqueda de hiperparámetros en los pasos de pre procesamiento (vectorización, tfidf y cambio de base)\n",
    "# será común a todos los modelos en el Grid Search.\n",
    "pre_processing = {\n",
    "                'vectorizador__lowercase': [True],\n",
    "                'vectorizador__strip_accents': ['unicode'],   \n",
    "                'vectorizador__ngram_range': [(1,3)],\n",
    "                'vectorizador__min_df': [0.001],\n",
    "                'vectorizador__tokenizer': [porter_tokenizer], \n",
    "                # Se elije porter para realizar la tokenización, debido a que las stopwords que sobreajustan fueron agregadas\n",
    "                # ya habiendose realizado el proceso de stem, y requiere un nuevo análisis de Feature importance para Lancaster\n",
    "                'cambio_base': [None,SVD_n_components(n_components_50_var)]\n",
    "                 }\n",
    "\n",
    "param_grid = [\n",
    "              {**pre_processing,\n",
    "               **{'classifier': [LogisticRegression()],\n",
    "                  'classifier__penalty': ['elasticnet', 'none'],\n",
    "                  'classifier__C': [ 1, 4, 7]}},              \n",
    "\n",
    "              {**pre_processing,\n",
    "               **{'classifier': [MultinomialNB()]}},\n",
    "              \n",
    "              {**pre_processing,\n",
    "               **{'classifier': [AdaBoostClassifier()],\n",
    "                  'classifier__n_estimators': [25, 50],\n",
    "                  'classifier__base_estimator': [DecisionTreeClassifier()],\n",
    "                  'classifier__base_estimator__criterion': ['gini'],\n",
    "                  'classifier__base_estimator__max_depth': [5],\n",
    "                  'classifier__base_estimator__max_features': [4000] } } \n",
    "                  # Se deja 4000 porque de forma simple no se logro tomar el valor de features vectorizadas.\n",
    "                  # Usualmente estaba entre 9000 a 10000 features, con 1000 records.\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f208528",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "param_grid_combinations = ParameterGrid(param_grid)\n",
    "len_param_grid = len(param_grid_combinations)\n",
    "print(f'La cantidad de combinaciones que debe realizar el GridSearchCV es de {len_param_grid}, multiplicado por {n_folds} que es el número de Folds, se obtiene un total de {len_param_grid * n_folds} posibilidades.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c807231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda demora 70 mins en ejecutarse\n",
    "\n",
    "with parallel_backend('threading', n_jobs = -1):\n",
    "    folds=StratifiedKFold(n_splits = n_folds, shuffle = True)\n",
    "    grid = GridSearchCV(estimator = pipeline,param_grid = param_grid, cv = folds, verbose = 1)\n",
    "    grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e96cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26a0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa69ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363ed1f",
   "metadata": {},
   "source": [
    "### Grid Search Testing\n",
    "Se realiza testing final con dataset completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = grid.predict(X_test)\n",
    "confusion_grid = confusion_matrix(y_test, y_pred_grid)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y_test, y_pred_grid))\n",
    "print(f'El modelo obtuvo un accuracy de {accuracy_score(y_pred_grid, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se re\n",
    "X = data['text']\n",
    "y = data['real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018561b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = grid.predict(X)\n",
    "confusion_grid = confusion_matrix(y, y_pred_grid)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, \n",
    "yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y, y_pred_grid))\n",
    "print(f'El modelo obtuvo un accuracy de {accuracy_score(y_pred_grid, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8195ca",
   "metadata": {},
   "source": [
    "### Pipeline con título de las noticias (sin el contenido del texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000 \n",
    "data_sample = data.sample(sample_size)\n",
    "X_title = data_sample['title']\n",
    "y_title = data_sample['real']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid_title = grid.predict(X_title)\n",
    "confusion_grid = confusion_matrix(y_title, y_pred_grid_title)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y_title, y_pred_grid_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d57f4c",
   "metadata": {},
   "source": [
    "El modelo no predice acertadamente si una noticia es verdadera o falsa sólamente basándose en el título. Por motivos que no logramos identificar clasifica a casi todos los títulos como si fuerean como noticias falsas. Quizás cómo los títulos tienden a ser sensacionalistas y a exagerar el contenido para cautivar la atención utilizan palabras más similares al texto de las noticias falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86eff5",
   "metadata": {},
   "source": [
    "<a id=\"section_pickle\"></a> \n",
    "\n",
    "## Exportación de mejor modelo entrenado\n",
    "Se realiza con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93953ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura del modelo\n",
    "with open('./pickle_model/fakenews_model.pkl', 'wb') as f_model:\n",
    "    pickle.dump(grid.best_estimator_, f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97017609",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lectura del modelo\n",
    "with open('./pickle_model/fakenews_model.pkl', 'rb') as f_model:\n",
    "        fakenews_model = pickle.load(f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura del GridSearch\n",
    "with open('./pickle_model/grid_search.pkl', 'wb') as f_grid:\n",
    "    pickle.dump(grid, f_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = fakenews_model.predict(X_test)\n",
    "confusion_grid = confusion_matrix(y_test, y_pred_grid)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales'), plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y_test, y_pred_grid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
