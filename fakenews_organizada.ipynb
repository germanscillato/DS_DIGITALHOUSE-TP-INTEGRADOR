{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a31fb20",
   "metadata": {},
   "source": [
    "TRABAJO PRACTICO - Integrador: GRUPO 8\n",
    "#Autores:\n",
    "\n",
    "- Choconi Lucas\n",
    "- Berra Eliel\n",
    "- Mina Federico\n",
    "- Scillato German"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f532a",
   "metadata": {},
   "source": [
    "## Importaciones y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- selenium\n",
    "- webdriver-manager\n",
    "- bs4\n",
    "- xgboost\n",
    "\n",
    "TERMINAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c4cf7",
   "metadata": {},
   "source": [
    "#### Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba54bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import time\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "\n",
    "import requests\n",
    "import warnings\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7100ab",
   "metadata": {},
   "source": [
    "#### Configuración Gráficos y Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caabd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración Graficos \n",
    "\n",
    "\n",
    "myColors = ((0.90, 0.96, 1, 1), (0.70, 0.87, 1, 1), (0, 0.40, 0.75, 0.88))\n",
    "cmap = LinearSegmentedColormap.from_list('Custom', myColors,10)\n",
    "\n",
    "# Fuente de titulos de gráficos\n",
    "font = {'color':  'darkred',\n",
    "        'weight': 'normal',\n",
    "        'size': 16,\n",
    "        }\n",
    "\n",
    "# Función para graficos de WorldCloud\n",
    "\n",
    "def blue_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "    \"\"\"Argumentos de función necesarias para metodo worldcloud\"\"\"\n",
    "    blues = \"hsl(215,100%%, %d%%)\" % np.random.choice(np.arange(25, 65, 5, dtype=int))\n",
    "    reds = \"hsl(0,100%%, %d%%)\" % np.random.choice(np.arange(40, 80, 5, dtype=int))\n",
    "    choice = np.random.choice([0, 1])\n",
    "    if choice == 0:\n",
    "        return blues\n",
    "    else:\n",
    "        return reds\n",
    "\n",
    "\n",
    "def graficos_manos(fake,true):\n",
    "    \"\"\"Función para graficar worldcloud en forma de manos,\n",
    "    falsas y verdaderas. Variables: worldcloud falsa y verdadera\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2,figsize=(20,10))  \n",
    "\n",
    "    axs[1].imshow(fake.recolor(color_func=blue_color_func, random_state=3),\n",
    "                interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[1].set_title('FAKE' , fontdict = font)\n",
    "    axs[0].imshow(true.recolor(color_func=blue_color_func,random_state=3),\n",
    "                interpolation=\"bilinear\")\n",
    "    axs[0].axis(\"off\")\n",
    "    axs[0].set_title('TRUE' , fontdict = font)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de imagenes para graficos de manos.\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else os.getcwd()\n",
    "mask_pos = np.array(Image.open(path.join(d, \"./data/Thumbs.png\")))\n",
    "mask_neg = np.array(Image.open(path.join(d, \"./data/Thumbsdw.png\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378fd10",
   "metadata": {},
   "source": [
    "## LOGICA DE DESARROLLO DEL MODELO\n",
    "Se busco el conjunto de stopwords siguiendo el siguiente esquema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6797cd74",
   "metadata": {},
   "source": [
    "![Diagrama](./data/fake_d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1136dcd",
   "metadata": {},
   "source": [
    "## Limpieza de datos\n",
    "Importación y limpieza del \n",
    "[Dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24083b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se une el dataset, con append entre True y Fake.\n",
    "\n",
    "file_names_true = pd.read_csv('./data/True.csv')\n",
    "file_names_true['real'] = 'True'\n",
    "file_names_fake = pd.read_csv('./data/Fake.csv')\n",
    "file_names_fake['real'] = 'Fake'\n",
    "data = file_names_fake.append(file_names_true, ignore_index=True)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unimos el título y el contenido del artículo en una sola columna\n",
    "data['text'] = data['title'] + ' --- ' + data['text'] \n",
    "\n",
    "# Ejemplo de 1 texto del dataset.\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "  print(data.head(1))\n",
    "data = data.drop(columns = 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d1791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algunas filas contienen datos erróneos que impiden transformar la columna 'date' a formato fecha\n",
    "wrong_row = data['date'] == 'https://100percentfedup.com/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier/'\n",
    "data[wrong_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las filas que no pueden ser correctamente transformadas a datetime\n",
    "# serán utilizadas en una máscara booleana para poder eliminarlas\n",
    "\n",
    "data['date'] = data['date'].apply(lambda x: pd.to_datetime(x,\n",
    "                                                    infer_datetime_format = True)\\\n",
    "                                                    if re.search('[a-z-A-Z]+ [0-9]+, [0-9]{4}', x) else  np.NaN)\n",
    "delete_mask = data['date'].notna()\n",
    "data = data[delete_mask]\n",
    "data = data.reset_index()\n",
    "data['date'] = data['date'].astype('datetime64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6013763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los datos en las filas que contienen noticias falsas de las que son reales\n",
    "data_real = data[data['real'] == 'True']\n",
    "\n",
    "data_fake = data[data['real'] == 'Fake']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a762ad7",
   "metadata": {},
   "source": [
    "# VALIDAR CON ELIEL O BORRAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Lista con todas las palabras en inglés https://github.com/dwyl/english-words/blob/master/words.txt\n",
    "all_eng_words = pd.read_csv('./data/all_english_words.csv')\n",
    "all_eng_words = pd.Series(all_eng_words.iloc[:,0])\n",
    "all_eng_words = all_eng_words.apply(lambda x: x.lower() if type(x) == str else x)\n",
    "\n",
    "def words_percentage(text):\n",
    "    # Devuelve el porcentaje de palabras que posee un texto que están dentro de un diccionario\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words] \n",
    "    words_in_dict = [word for word in words if word in all_eng_words.values]\n",
    "    words_in_dict_percentage = len(words_in_dict) * 100 / len(words)\n",
    "    return words_in_dict_percentage\n",
    "\n",
    "tqdm.pandas()\n",
    "data['words_in_dict_pct'] = data['text'].progress_apply(words_percentage)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62d9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(rc={'figure.figsize':(15, 4)})\n",
    "sns.histplot(data = data, x = 'subject', hue = 'real' )\n",
    "\n",
    "plt.ylabel('Cantidad de noticias')\n",
    "plt.xlabel('Tipo de noticias')\n",
    "plt.title(\"Balance del Dataset\", fontdict= font)\n",
    "plt.figure(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b8625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como hay valores diferentes en la columna \"subject\" para las noticias reales y las falsas borramos esa columna\n",
    "data = data.drop(columns = 'subject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c55e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hacemos un gráfico mostrando el conteo de cuántas noticias falsas y cuántas verdaderas se publicaron \n",
    "# por mes en el dataset \n",
    "ax = data_real['date'].groupby([data_real[\"date\"].dt.year,\n",
    "                             data_real[\"date\"].dt.month]).count().plot.line(label = 'Real news')\n",
    "\n",
    "ax = data_fake['date'].groupby([data_fake[\"date\"].dt.year,\n",
    "                            data_fake[\"date\"].dt.month]).count().plot.line(label = 'Fake news')\n",
    "ax.set(xlabel=None)\n",
    "ax.set_title('Linea Temporal Noticias' , fontdict = font)\n",
    "ax.set_xlabel('Año / Mes')\n",
    "ax.set_ylabel('Cantidad Noticias')\n",
    "ax.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a990792",
   "metadata": {},
   "source": [
    "El pico de noticias verdaderas publicadas en Noviembre del 2016 coincide con las elecciones presidenciales de Estados Unidos\n",
    "\n",
    "Se verifica limpieza de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d683f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El dataset no valores nulos así que damos por finalizada la limpieza\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4665abb",
   "metadata": {},
   "source": [
    "# Pre-procesamiento\n",
    "\n",
    "A partir del wordcloud se analiza el Dataset y se verifica inicialmente posible palabras a descartar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a436a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed12241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wordcloud_fake = WordCloud(stopwords=stopwords_en,\n",
    "            mask=mask_neg, \n",
    "            max_words=500,\n",
    "            background_color=\"white\").generate(data[data['real']=='Fake'].text.str.cat(sep=' '))\n",
    "wordcloud_true = WordCloud(stopwords=stopwords_en,\n",
    "            mask=mask_pos,\n",
    "            max_words=500,\n",
    "            background_color=\"white\").generate(data[data['real']=='True'].text.str.cat(sep=' '))\n",
    "\n",
    "\n",
    "graficos_manos(fake = wordcloud_fake,true = wordcloud_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a361cf3",
   "metadata": {},
   "source": [
    "Se divide el dataset en train - test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12989a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sample_size = 2000 # Algunas operaciones se demoran mucho tiempo, tamaño pequeño es para agilizarlas\n",
    "\n",
    "data_sample = data.sample(testing_sample_size)\n",
    "X = data_sample['text']\n",
    "y = data_sample['real']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f94f6",
   "metadata": {},
   "source": [
    "Para obtener la raiz de las palabras se utiliza [Porter Stemmer](https://www.nltk.org/_modules/nltk/stem/porter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509dda68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se utiliza este stemmer por default, y al obtenerse buenos resultados iniciales, se continua trabajando en el análisis\n",
    "# de las features importance. \n",
    "# Es por esto que se descarta agregar otro stemmer al pipeline, ya que requeriria una nueva puesta de análisis de dichas palabras.\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stopwords_en_porter = [porter.stem(x) for x in stopwords_en]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrego Stopwords específicas del dataset.\n",
    "def append_stopword(listado_stopwords, stopwords_agregar):\n",
    "    for stopwords in stopwords_agregar:\n",
    "        listado_stopwords.append(stopwords)\n",
    "    return listado_stopwords\n",
    "\n",
    "stopwords_agregar = ['reuter','said','Reuters','via','imag','https','com','one',\n",
    "'u','also','would','featur','pic','us','wednesday','friday','monday','tuesday',\n",
    "'saturday','sunday','thursday','getti','read','gop','watch','donald','trump',\n",
    "'hillari','mr','accord','america','seem','youtub','21st',\n",
    "'video' , 'http' , 'like' , 'obama' , 'minist' , 'washington' , 'know' , ]\n",
    "\n",
    "stopwords_en_porter = append_stopword(stopwords_en_porter ,stopwords_agregar )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de función para el tokenizer\n",
    "def porter_tokenizer(text):\n",
    "    ''' Esta función se utiliza cuando se sobre escribe el proceso\n",
    "    de tokenización en la clase CountVectorizer para que se pueda\n",
    "     obtener también la raíz de la palabra.'''\n",
    "    words = word_tokenize(text)\n",
    "    words = [re.sub('[^A-Za-z0-9]+', '', word) for word in words]  # remover los caracteres especiales\n",
    "    words = [word.replace(' ', '') for word in words] #remover los espacios en blanco\n",
    "    words = [word for word in words if word != '']\n",
    "    words = [PorterStemmer().stem(word) for word in words] # obtener la raiz de las palabras\n",
    "    words = [word for word in words if word not in stopwords_en_porter] # remover las stopwords\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa3faf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# La ventaja de este CountVectorizer customizado es que permite obtener la raíz de la palabra\n",
    "customized_vectorizer= CountVectorizer(lowercase = True,\n",
    "                                        strip_accents='unicode', \n",
    "                                        tokenizer = porter_tokenizer,\n",
    "                                        ngram_range = (1, 3),\n",
    "                                        min_df = 3)\n",
    "\n",
    "customized_vectorizer_train = customized_vectorizer.fit_transform(X_train)\n",
    "customized_vectorizer_test = customized_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f863532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construcción de matrices tfidf\n",
    "tfidf_matrix_train = TfidfTransformer().fit_transform(customized_vectorizer_train)\n",
    "tfidf_matrix_test = TfidfTransformer().fit_transform(customized_vectorizer_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab57ee0",
   "metadata": {},
   "source": [
    "### Cambio de Base\n",
    "\n",
    "##### Evaluación del impacto con distintas cantidades de componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de función para cambio de base\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) -> int: \n",
    "    ''' Esta función se utiliza para hacer un cambio de base y lograr explicar\n",
    "    el porcentaje de los datos que se pasa en el argumento goal_var.\n",
    "    Fuente https://chrisalbon.com/code/machine_learning/feature_engineering/select_best_number_of_components_in_tsvd/\n",
    "    Fue modificada para que hiciera un gráfico de la varianza explicativa del\n",
    "    modelo de acuerdo al número de componentes'''\n",
    "    # Set initial variance explained so far\n",
    "    total_variance = 0.0\n",
    "    # Set initial number of features\n",
    "    n_components = 0\n",
    "    # Listas del nuḿero de componentes y de la explicación de varianza alcanzada para poder graficarlas\n",
    "    num_components = list(range(1, len(var_ratio) + 1))\n",
    "    variances = []\n",
    "    # For the explained variance of each feature:\n",
    "    found_n_components = False\n",
    "    for explained_variance in var_ratio:\n",
    "        # Add the explained variance to the total\n",
    "        total_variance += explained_variance\n",
    "        # Add one to the number of components\n",
    "        n_components += 1\n",
    "        # Agregar la varianza explicada al listado \n",
    "        variances.append(total_variance)\n",
    "        # If we reach our goal level of explained variance and we haven't reached our desired variance\n",
    "        if total_variance >= goal_var and found_n_components == False:\n",
    "            desired_n_components = n_components\n",
    "            found_n_components = True\n",
    "            desired_variance = total_variance\n",
    "    plt.plot(num_components, variances )\n",
    "    plt.scatter(desired_n_components, desired_variance, c = 'r')\n",
    "    plt.xlabel('Número de componentes')\n",
    "    plt.ylabel('Razón de la explicación de la varianza')\n",
    "    plt.hlines(desired_variance, 0, desired_n_components, 'r', 'dashed')\n",
    "    plt.vlines(desired_n_components, 0, desired_variance, 'r', 'dashed')\n",
    "    plt.title(\"Comparativa Varianza vs Número componentes \",fontdict=font)\n",
    "        \n",
    "    # Return the desired number of components. \n",
    "    # Tipo de variable devuelta definido por \" -> int \" PEP3107\n",
    "    return desired_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a52c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación de cambio de base para 3 threshold\n",
    "# Evaluamos cómo responde el cambio de base cuando seleccionamos una explicación\n",
    "# de la varianza alta (0.9), media (0.5) o baja (0.1)\n",
    "desired_threshold = [0.9 , 0.5 , 0.1]\n",
    "\n",
    "for thres in desired_threshold:\n",
    "    # Tener en cuenta que si el número de filas de la matrix esparsa es menor al número de componentes elegido\n",
    "    # como hiperparámetro, una vez que se haya realizado la transformación SVD el número de componentes \n",
    "    # obtenido será igual al número de filas de la matriz esparsa, y no será el número de componentes\n",
    "    # elegido como hiperpárametro\n",
    "    test_svd = TruncatedSVD(n_components = tfidf_matrix_train.shape[1] - 1);\n",
    "    test_svd.fit(tfidf_matrix_train)\n",
    "    tsvd_var_ratios = test_svd.explained_variance_ratio_\n",
    "    desired_threshold = thres\n",
    "    n_components = select_n_components(tsvd_var_ratios, desired_threshold)\n",
    "\n",
    "    print(f'Para Threshold {thres} número de componentes después del cambio de base: {n_components}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ef46e",
   "metadata": {},
   "source": [
    "##### Aplicacion SVD\n",
    "\n",
    "Se concluye que se tiene una performance aceptable aunque solo se capture el 10% de la varianza del dataset. Por lo cual, en el pipeline se incluiran los 3 threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa245fdd",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Esta etapa de la construcción del modelo, se hizo de forma iterativa, verificando palabras que no tuvieran sentido real en la predicción de noticias falsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fdd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se realiza la vectorización con el dataset completo.\n",
    "customized_vectorizer_feature= CountVectorizer(lowercase = True,\n",
    "                                        strip_accents='unicode', \n",
    "                                        tokenizer = porter_tokenizer,\n",
    "                                        ngram_range = (1, 3),\n",
    "                                        min_df = 0.002,\n",
    "                                        max_df= 0.9)\n",
    "\n",
    "\n",
    "customized_vectorizer_feature.fit(data.text)\n",
    "customized_vectorizer_matrix = customized_vectorizer_feature.transform(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70e1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix_feature = TfidfTransformer().fit_transform(customized_vectorizer_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de modelo Tree para evaluar peso de cada palabra.\n",
    "dt = tree.DecisionTreeClassifier(criterion='gini',\n",
    "max_depth=4,min_samples_leaf=2)\n",
    "dt.fit(tfidf_matrix_feature, data[\"real\"])\n",
    "importancia_features = pd.DataFrame(dt.feature_importances_,\n",
    " index = customized_vectorizer_feature.get_feature_names(),\n",
    " columns=['importancia'])\n",
    "importancia_features_sort = importancia_features.sort_values('importancia', ascending=False)\n",
    "print(importancia_features_sort[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a001533",
   "metadata": {},
   "source": [
    "### Robustez del modelo - Influencia de stopwords\n",
    "\n",
    "Se detecto que el modelo sobreajuste para el dataset inicial, si no se retira las stopwords definidas. A continuación se muestra los resultados del modelo incluyendo dichas palabras. Se utiliza el modelo de \"Tree\" como ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se redefine x_train y x_test\n",
    "testing_sample_size = 1000 # Algunas operaciones se demoran mucho tiempo, tamaño pequeño es para agilizarlas\n",
    "\n",
    "data_sample = data.sample(testing_sample_size)\n",
    "X = data_sample['text']\n",
    "y = data_sample['real']\n",
    "X_train_completo, X_test_completo, y_train_completo, y_test_completo = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700705a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se redefine stopwords y se genera nuevamente la vectorización, y la matriz tfidf\n",
    "\n",
    "\n",
    "stopwords_en_porter = [porter.stem(x) for x in stopwords_en]\n",
    "customized_vectorizer_feature= CountVectorizer(lowercase = True,\n",
    "                                        strip_accents='unicode', \n",
    "                                        tokenizer = porter_tokenizer,\n",
    "                                        ngram_range = (1, 3),\n",
    "                                        min_df = 3)\n",
    "\n",
    "\n",
    "\n",
    "customized_vectorizer_matrix = customized_vectorizer_feature.fit_transform(X_train_completo)\n",
    "\n",
    "customized_vectorizer_test = customized_vectorizer_feature.transform(X_test_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09826d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_matrix_train_completo = TfidfTransformer().fit_transform(customized_vectorizer_matrix)\n",
    "\n",
    "tfidf_matrix_test_completo = TfidfTransformer().fit_transform(customized_vectorizer_test) \n",
    "\n",
    "\n",
    "X_train_completo= tfidf_matrix_train_completo\n",
    "X_test_completo = tfidf_matrix_test_completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47be6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modelo ejemplo para mostrar sobre ajuste.\n",
    "my_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "my_tree.fit(X_train_completo , y_train_completo )\n",
    "print(\"Score modelo con stopwords: \", round(my_tree.score(X_test_completo ,\n",
    "                                            y_test_completo )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tree = confusion_matrix(y_test_completo , my_tree.predict(X_test_completo ))\n",
    "\n",
    "axis_labels = np.sort(y_test.unique())\n",
    "sns.heatmap(cm_tree, xticklabels=axis_labels,yticklabels=axis_labels,\n",
    "             cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas');\n",
    "plt.figure(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f2aea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(my_tree,feature_names = customized_vectorizer_feature.get_feature_names(),\n",
    "                filled=True,rounded=True, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9afa2",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c362c2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAN CON SVD???? O SIN SVD???\n",
    "\n",
    "\n",
    "X_train = tfidf_matrix_train\n",
    "X_test = tfidf_matrix_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72ce672",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b16f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "NB_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9fc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "axis_labels = np.sort(y_test.unique())\n",
    "cm_nb = confusion_matrix(y_test, NB_model.predict(X_test))\n",
    "sns.heatmap(cm_nb, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc285b",
   "metadata": {},
   "source": [
    "#### Modelo de Reg. Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf5edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LogisticRegression(C = 7, solver=\"saga\", l1_ratio=0.5 ,\n",
    "                                 penalty=\"elasticnet\", n_jobs=-1)\n",
    "reg_model.fit(X_train, y_train)\n",
    "reg_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rlog = confusion_matrix(y_test, reg_model.predict(X_test))\n",
    "sns.heatmap(cm_rlog, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "             cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07bd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Coef_reg_model = pd.DataFrame(np.transpose(reg_model.coef_), index=customized_vectorizer.get_feature_names(),\n",
    " columns=['Coef'])\n",
    "Coef_reg_model['coef_abs'] = abs(Coef_reg_model['Coef'])\n",
    "Coef_reg_model = Coef_reg_model.reset_index(-1)\n",
    "Coef_reg_model['words'] = Coef_reg_model['index'] \n",
    "Coef_reg_model['freq'] = (round(Coef_reg_model['coef_abs'] / Coef_reg_model.coef_abs.sum()* Coef_reg_model.shape[0] * 1000 , 0)).apply(lambda x: int(x))\n",
    "Coef_reg_model['odds_direction'] = Coef_reg_model['Coef'].apply(lambda x: 'Negative' if x<0 else 'Positive')\n",
    "\n",
    "Coef_reg_model_positive = Coef_reg_model[Coef_reg_model['odds_direction']=='Positive']\n",
    "Coef_reg_model_positive = Coef_reg_model_positive.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)\n",
    "Coef_reg_model_negative = Coef_reg_model[Coef_reg_model['odds_direction']=='Negative']\n",
    "Coef_reg_model_negative = Coef_reg_model_negative.drop(['Coef' ,'coef_abs', 'index', 'odds_direction'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70c29ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuples_pos = Coef_reg_model_positive.set_index('words').T.to_dict('index')\n",
    "tuples_neg = Coef_reg_model_negative.set_index('words').T.to_dict('index')\n",
    "\n",
    "wordcloud_positive = WordCloud(max_words=500, mask=mask_pos,\n",
    "        background_color=\"white\").generate_from_frequencies(dict(tuples_pos['freq']))\n",
    "wordcloud_negative = WordCloud(max_words=500, mask=mask_neg,\n",
    "        background_color=\"white\").generate_from_frequencies(dict(tuples_neg['freq']))\n",
    "\n",
    "graficos_manos(fake = wordcloud_fake,true = wordcloud_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51533a50",
   "metadata": {},
   "source": [
    "## Modelo de Reg. Logistica Truncando palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9bbc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Esta bien que sea tfidf test????\n",
    "\n",
    "test_svd = TruncatedSVD(n_components = X_test.shape[1] - 1);\n",
    "test_svd.fit(X_test)\n",
    "tsvd_var_ratios = test_svd.explained_variance_ratio_\n",
    "desired_threshold_90 = 0.90\n",
    "desired_threshold_50 = 0.50\n",
    "desired_threshold_10 = 0.10\n",
    "n_components_90 = select_n_components(tsvd_var_ratios, desired_threshold_90)\n",
    "n_components_50 = select_n_components(tsvd_var_ratios, desired_threshold_50)\n",
    "n_components_10 = select_n_components(tsvd_var_ratios, desired_threshold_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "svd_90 = TruncatedSVD(n_components = n_components_90)\n",
    "svd_50 = TruncatedSVD(n_components = n_components_50)\n",
    "svd_10 = TruncatedSVD(n_components = n_components_10)\n",
    "\n",
    "X_train_svd_90 = svd_90.fit_transform(X_train)\n",
    "X_test_svd_90 = svd_90.transform(X_test)\n",
    "X_train_svd_50 = svd_50.fit_transform(X_train)\n",
    "X_test_svd_50 = svd_50.transform(X_test)\n",
    "X_train_svd_10 = svd_10.fit_transform(X_train)\n",
    "X_test_svd_10 = svd_10.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f605964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_svd = [[X_train_svd_90,X_test_svd_90],[X_train_svd_50,X_test_svd_50],[X_train_svd_10,X_test_svd_10]]\n",
    "scores = []\n",
    "models = []\n",
    "for svd in X_svd:\n",
    "\n",
    "    reg_model_svd = LogisticRegression(C = 8, solver=\"newton-cg\", penalty=\"l2\")\n",
    "    models.append(reg_model_svd.fit(svd[0], y_train))\n",
    "    scores.append(reg_model_svd.score(svd[1], y_test))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs, axs = plt.subplots(1,3, figsize= (15,5))\n",
    "title = [90,50,10]\n",
    "for i in range(3):\n",
    "    \n",
    "    cm_rlog_svd = confusion_matrix(y_test, models[i].predict(X_svd[i][1]))\n",
    "    sns.heatmap(cm_rlog_svd,xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "                cbar=False, annot=True, cmap=cmap, fmt='d', ax = axs[i],\n",
    "                linecolor = \"w\" , linewidths = .5,square=True)\n",
    "    axs[i].set_title(label = f'SVD Threshold {title[i]}' , \n",
    "    loc ='left', fontdict = font )\n",
    "    axs[0].set_ylabel('Etiquetas reales')\n",
    "    axs[i].set_xlabel('Etiquetas predichas')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acace5",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b612e1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
    "my_tree.fit(X_train, y_train)\n",
    "my_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tree = confusion_matrix(y_test, my_tree.predict(X_test))\n",
    "sns.heatmap(cm_tree, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "            cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea932f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "tree.plot_tree(my_tree,feature_names = customized_vectorizer.get_feature_names(),\n",
    "                filled=True,rounded=True, fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b675f193",
   "metadata": {},
   "source": [
    "## Boosting de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc195ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifier = ExtraTreesClassifier(n_estimators=100, \n",
    "                                      criterion='gini', \n",
    "                                      max_depth = 4, \n",
    "                                      bootstrap=True, \n",
    "                                      n_jobs = -1, \n",
    "                                      random_state = 127,\n",
    "                                      max_samples= 0.3)\n",
    "\n",
    "boost_tree = AdaBoostClassifier(base_estimator = base_classifier, \n",
    "                            n_estimators = 200,\n",
    "                            learning_rate = 0.8,                                       \n",
    "                            random_state = 127)\n",
    "\n",
    "boost_tree.fit(X_train, y_train) \n",
    "boost_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_boost_tree = confusion_matrix(y_test, boost_tree.predict(X_test))\n",
    "sns.heatmap(cm_boost_tree, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe20d5",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos - ROC CURVE  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675e5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Calculo de probabilidad de los distintos modelos\n",
    "y_pred_boost = boost_tree.predict_proba(X_test)\n",
    "y_pred_my_tree = my_tree.predict_proba(X_test)\n",
    "y_pred_reg_model = reg_model.predict_proba(X_test)\n",
    "y_pred_NB = NB_model.predict_proba(X_test)\n",
    "\n",
    "models = models.append(boost_tree,)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd9a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def ROC_curve(modelos, X_test , y_test):\n",
    "    \"Calcula las probabilidades y grafica la ROC Curve de los modelos\n",
    "    en lista 'Modelos'. \"\n",
    "    for model in modelos:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        fpr_log, tpr_log, thr_log = roc_curve(y_test, y_pred_proba[:,1])\n",
    "        df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "        plt.plot(df['fpr'],df['tpr'], label='Modelo')\n",
    "    \n",
    "    plt.axis([0, 1.01, 0, 1.01]); plt.legend()\n",
    "    plt.xlabel('1 - Specificty'); plt.ylabel('TPR / Sensitivity'); plt.title('ROC Curve')\n",
    "    plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "    plt.show()\n",
    "\n",
    "ROC_curve(modelos = models, X_test = X_test , y_test = y_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88b1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTO LO AGREGAMOS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fa610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train_dense = pd.DataFrame(X_train.todense(), columns=count_vectorizer.get_feature_names_out())\n",
    "X_train_explainer = np.array(X_train_dense)\n",
    "explainer = LimeTabularExplainer(X_train_explainer, \n",
    "                                 mode = \"classification\",\n",
    "                                 training_labels = y_train,\n",
    "                                 feature_names = count_vectorizer.get_feature_names_out(),\n",
    "                                 discretize_continuous=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2e7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"data_row = np.array(X_test[0].todense())[0]\n",
    "\n",
    "explanation = explainer.explain_instance(data_row, reg_model.predict_proba, num_features=10)\n",
    "explanation.as_pyplot_figure()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96366c48",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sample_size = 1000 # Algunas operaciones se demoran mucho tiempo, tamaño pequeño es para agilizarlas\n",
    "\n",
    "data_sample = data.sample(testing_sample_size)\n",
    "X = data_sample['text']\n",
    "y = data_sample['real']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasos = [('vectorizador', CountVectorizer(lowercase = True, \n",
    "                                            strip_accents='unicode',\n",
    "                                          tokenizer = porter_tokenizer)), \n",
    "         ('tfidf', TfidfTransformer()), \n",
    "         ('cambio_base', TruncatedSVD(n_components = n_components_90)),\n",
    "         ('modelo', LogisticRegression())]\n",
    "pipeline = Pipeline(pasos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279bb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{'vectorizador__ngram_range': [(1, 4)],\n",
    "               'vectorizador__min_df': np.arange(0.01,0.11,0.02),\n",
    "               'vectorizador__max_df': np.arange(0.8,1,0.05) ,\n",
    "               'tfidf__norm': ['l1', 'l2'],\n",
    "               'cambio_base': [TruncatedSVD()],\n",
    "               'cambio_base__n_components': [n_components_10,\n",
    "                                            n_components_50,\n",
    "                                            n_components_90],\n",
    "               'modelo': [AdaBoostClassifier(), LogisticRegression()],\n",
    "                #ExtraTreesClassifier() , ()]\n",
    "               }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apróximadamente toma 10 min la ejecución con un sample de 1000 records y param_grid2\n",
    "folds=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "grid = GridSearchCV(estimator = pipeline,param_grid = param_grid, cv = folds)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e96cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26a0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa69ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363ed1f",
   "metadata": {},
   "source": [
    "##### Grid Search Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1a5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = grid.predict(X_test)\n",
    "\"\"\"confusion = confusion_matrix(y_test, y_pred_grid)\n",
    "print(confusion)\"\"\"\n",
    "confusion_grid = confusion_matrix(y_test, y_pred_grid)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86eff5",
   "metadata": {},
   "source": [
    "## Exportación de mejor modelo entrenado\n",
    "Se realiza con pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93953ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escritura del modelo\n",
    "with open('fakenews_model.pkl', 'wb') as f_model:\n",
    "    pickle.dump(grid.best_estimator_, f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97017609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura del modelo\n",
    "with open('fakenews_model.pkl', 'rb') as f_model:\n",
    "        fakenews_model = pickle.load(f_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873e59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grid = fakenews_model.predict(X_test)\n",
    "\"\"\"confusion = confusion_matrix(y_test, y_pred_grid)\n",
    "print(confusion)\"\"\"\n",
    "confusion_grid = confusion_matrix(y_test, y_pred_grid)\n",
    "sns.heatmap(confusion_grid, xticklabels=axis_labels, yticklabels=axis_labels, cbar=False, annot=True, cmap=cmap, fmt='g')\n",
    "plt.ylabel('Etiquetas reales')\n",
    "plt.xlabel('Etiquetas predichas')\n",
    "print (classification_report(y_test, y_pred_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ae5f5",
   "metadata": {},
   "source": [
    "## WEB APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ce31929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'my__webapp' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on all addresses (0.0.0.0)\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://127.0.0.1:5017\n",
      " * Running on http://192.168.1.15:5017 (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [20/Jun/2022 21:54:05] \"GET /reg_model/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [20/Jun/2022 21:54:14] \"GET /reg_model/LONDON%20(Reuters)%20-%20Britain’s%20foreign%20minister%20Boris%20Johnson%20has%20arrived%20in%20the%20United%20States%20to%20meet%20close%20advisers%20to%20President-elect%20Donald%20Trump%20and%20senior%20Congressional%20leaders%20to%20discuss%20ties%20between%20the%20countries.%20Last%20June’s%20vote%20to%20leave%20the%20European%20Union%20has%20left%20Britain%20facing%20some%20of%20the%20most%20complicated%20negotiations%20since%20World%20War%20Two,%20with%20the%20country%20keen%20to%20deepen%20ties%20with%20the%20United%20States%20and%20other%20nations%20to%20show%20that%20Brexit%20will%20not%20diminish%20its%20standing%20in%20the%20world.%20Johnson’s%20visit,%20which%20was%20not%20flagged%20in%20advance,%20is%20part%20of%20Prime%20Minister%20Theresa%20May’s%20strategy%20to%20improve%20relations%20with%20Trump’s%20team%20after%20the%20president-elect%20irritated%20the%20government%20by%20saying%20that%20outspoken%20anti-EU%20campaigner%20Nigel%20Farage%20would%20be%20a%20good%20choice%20for%20Britain’s%20ambassador%20to%20Washington.%20%20May’s%20two%20most%20senior%20aides%20made%20a%20secret%20trip%20to%20the%20United%20States%20last%20month.%20“Following%20the%20successful%20meeting%20last%20month%20between%20the%20Prime%20Minister’s%20chiefs%20of%20staff%20and%20President-elect%20Donald%20Trump’s%20team,%20Foreign%20Secretary%20Boris%20Johnson%20is%20undertaking%20a%20short%20visit%20to%20the%20U.S.%20for%20meetings%20with%20close%20advisers%20to%20the%20president-elect%20and%20senior%20Congressional%20leaders,”%20a%20spokesman%20for%20Britain’s%20foreign%20ministry%20said%20in%20a%20statement%20on%20Sunday.%20“The%20discussions%20will%20be%20focused%20on%20UK-U.S.%20relations%20and%20other%20foreign%20policy%20matters.”%20May%20had%20told%20Sky%20News%20earlier%20in%20the%20day%20that%20she%20was%20sure%20that%20Britain%20and%20the%20United%20States%20would%20build%20on%20their%20close%20ties%20and%20that%20their%20“special%20relationship”%20would%20endure%20despite%20describing%20some%20of%20his%20comments%20about%20women%20as%20“unacceptable”.%20%22) HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Importar\n",
    "from flask import Flask\n",
    "from flask import render_template\n",
    "from utils import *\n",
    "\n",
    "app = Flask('my__webapp')\n",
    "\n",
    "def fn_select_proba(value=\"True\"):\n",
    "    if value == \"Fake\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "@app.route('/reg_model/<text>', methods=['GET'])\n",
    "def mi_modelo(text):\n",
    "    # Lectura del modelo\n",
    "    with open('fakenews_model.pkl', 'rb') as f_model:\n",
    "        fakenews_model = pickle.load(f_model)\n",
    "\n",
    "\n",
    "    text_df = pd.Series(data = text)\n",
    "\n",
    "    result = {\"predict\": fakenews_model.predict(text_df)[0]}\n",
    "    result_proba = {\"predict\": np.round(fakenews_model.predict_proba(text_df),2)}\n",
    "    \n",
    "    return render_template('predict_page.html', predict=result[\"predict\"] ,\n",
    "     predict_proba=result_proba[\"predict\"][0])  \n",
    "\n",
    "@app.route('/reg_model/', methods=['GET'])\n",
    "def home():\n",
    "    return render_template('home.html')      \n",
    "\n",
    "@app.route(\"/test\", methods=['GET'])\n",
    "def hello():\n",
    "    alive = \"La app está viva\"\n",
    "    return alive\n",
    "\n",
    "\n",
    "app.run(host='0.0.0.0', port = 5017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92369685",
   "metadata": {},
   "source": [
    "Abrimos en un browser esta url (GET)\n",
    "\n",
    "[Test WEB APP](http://localhost:5017/test)\n",
    "\n",
    "\n",
    "[FAKE NEWS PREDICTION](http://localhost:5017/reg_model/)\n",
    "\n",
    "http://localhost:5017/test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b0819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba0e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f75a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127a22ef",
   "metadata": {},
   "source": [
    "## Pipeline con título de las noticias (SIN contenido del texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer el pipeline sólamente con los títulos de las noticias para ver qué pasa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f1402",
   "metadata": {},
   "source": [
    "## A traves de webscraping se buscan noticias para obtener más datos de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dde34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 102.0.5005\n",
      "[WDM] - Get LATEST chromedriver version for 102.0.5005 google-chrome\n",
      "[WDM] - Driver [C:\\Users\\Usuario\\.wdm\\drivers\\chromedriver\\win32\\102.0.5005.61\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "wd = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=chrome_options)\n",
    "\n",
    "def get_text_from_url_bbc(url):\n",
    "    wd.get(url)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    soup = BeautifulSoup(wd.page_source)\n",
    "    text = soup.find_all('div', {'data-component': 'text-block'}) #(?P<text>\\>(.*?)\\<)\n",
    "    text = reversed(text) \n",
    "    text2 = \"\"\n",
    "    for i in text:\n",
    "        try:\n",
    "            text2 = i.text+text2\n",
    "        except:\n",
    "            text2 = \"\"+text2\n",
    "    return text2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46b9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#https://newsapi.org/docs\n",
    "response = requests.get(\"https://newsapi.org/v2/top-headlines?sources=bbc-news&apiKey=1f83e742c8804de0a5d427510829f79b\")\n",
    "\n",
    "list_urls = []\n",
    "for i in range(0,len(response.json()['articles'])):\n",
    "    list_urls.append(response.json()['articles'][i]['url'])  \n",
    "    \n",
    "list_texts = []\n",
    "for i in range(0,len(list_urls)):\n",
    "    list_texts.append(get_text_from_url_bbc(list_urls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7deb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review_text, tokenizer, stemmer, stopwords):    \n",
    "    \n",
    "    #tokens (eliminamos todos los signos de puntuación)\n",
    "    words = tokenizer.tokenize(review_text)\n",
    "    \n",
    "    # stemming: raiz y minúsculas:\n",
    "    stem_words = [stemmer.stem(x) for x in words]\n",
    "    \n",
    "    # eliminamos stopwords (ya pasaron por stem)\n",
    "    clean_words = [x for x in stem_words if x not in stopwords]\n",
    "    \n",
    "    result = \" \".join(clean_words)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5471a944",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'customized_vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\python39\\virtual_env\\fakenews\\fakenews_organizada.ipynb Cell 105'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=3'>4</a>\u001b[0m data_texts_text \u001b[39m=\u001b[39m data_texts_text[data_texts_text\u001b[39m.\u001b[39mlen\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=5'>6</a>\u001b[0m clean_texts_test \u001b[39m=\u001b[39m data_texts_text\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: porter_tokenizer(x))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=7'>8</a>\u001b[0m X_test_sparse_texts \u001b[39m=\u001b[39m customized_vectorizer\u001b[39m.\u001b[39mtransform(clean_texts_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=8'>9</a>\u001b[0m X_test_texts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(X_test_sparse_texts\u001b[39m.\u001b[39mtodense(), \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=9'>10</a>\u001b[0m              columns \u001b[39m=\u001b[39m customized_vectorizer\u001b[39m.\u001b[39mget_feature_names()) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python39/virtual_env/fakenews/fakenews_organizada.ipynb#ch0000103?line=10'>11</a>\u001b[0m X_test_svd_texts \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39mtransform(X_test_texts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'customized_vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "data_texts_text = pd.DataFrame(list_texts, columns=['text'])\n",
    "data_texts_text['len'] = data_texts_text.text.apply(lambda x: len(x))\n",
    "data_texts_text['real'] = 'True'\n",
    "data_texts_text = data_texts_text[data_texts_text.len>0]\n",
    "\n",
    "clean_texts_test = data_texts_text.text.apply(lambda x: porter_tokenizer(x))\n",
    "\n",
    "X_test_sparse_texts = customized_vectorizer.transform(clean_texts_test)\n",
    "X_test_texts = pd.DataFrame(X_test_sparse_texts.todense(), \n",
    "             columns = customized_vectorizer.get_feature_names()) \n",
    "X_test_svd_texts = svd.transform(X_test_texts)\n",
    "\"\"\"X_test_bigram_sparse_texts = count_vectorizer_bigram.transform(clean_texts_test)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0731cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text['predict_reg_base'] = reg_model.predict(X_test_texts)\n",
    "data_texts_text['predict_prob_reg_base'] = reg_model.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text['predict_svd'] = reg_model_svd.predict(X_test_svd_texts)\n",
    "data_texts_text['predict_prob_svd'] = reg_model_svd.predict_proba(X_test_svd_texts).T[0]\n",
    "\n",
    "\"\"\"data_texts_text['predict_bigram'] = reg_model_bigram.predict(X_test_bigram_sparse_texts)\n",
    "data_texts_text['predict_prob_bigram'] = reg_model_bigram.predict_proba(X_test_bigram_sparse_texts).T[0]\n",
    "\"\"\"\n",
    "data_texts_text['predict_my_tree'] = my_tree.predict(X_test_texts)\n",
    "data_texts_text['predict_prob_my_tree'] = my_tree.predict_proba(X_test_texts).T[0]\n",
    "\n",
    "data_texts_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfe597",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_explainer = np.array(X_train)\n",
    "explainer = LimeTabularExplainer(X_train_explainer, \n",
    "                                 mode = \"classification\",\n",
    "                                 training_labels = y_train,\n",
    "                                 feature_names = X_train.columns,\n",
    "                                 discretize_continuous=False)\n",
    "\n",
    "#i = 13\n",
    "#data_row = np.array(X_test.iloc[i])\n",
    "data_row = np.array(X_test_texts.iloc[4])\n",
    "explanation = explainer.explain_instance(data_row, reg_model.predict_proba, num_features=10)\n",
    "explanation.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca96728",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts_text.text.iloc[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
